{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'E:\\\\UIT DATA SCIENCE\\\\data samples\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>caption_features</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>image_vector</th>\n",
       "      <th>image_text</th>\n",
       "      <th>object</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor([[ 8.7540e-02, -8.4067e-03, -5.7961e-01...</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "      <td>tensor([[ 7.2288e-01,  1.0506e-02, -7.5529e-02...</td>\n",
       "      <td>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...</td>\n",
       "      <td>tensor([[ 2.4169e-01,  7.7020e-02, -1.2376e-02...</td>\n",
       "      <td>tensor([[ 1.4588e-01, -1.4534e-02, -3.4795e-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor([[-1.4093e-01,  2.6814e-01,  6.6393e-02...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "      <td>tensor([[ 2.4603e-02,  7.0014e-01,  2.0763e-01...</td>\n",
       "      <td>tensor([[-2.9716e-01,  3.0908e-01, -1.9151e-01...</td>\n",
       "      <td>tensor([[ 2.8199e-01, -1.5914e-01, -1.4282e-01...</td>\n",
       "      <td>tensor([[-5.6289e-02,  4.7537e-01, -6.9459e-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>tensor([[ 2.9297e-02,  7.8513e-02,  1.9056e-02...</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "      <td>tensor([[ 2.8290e-01,  1.8930e-01, -7.7651e-02...</td>\n",
       "      <td>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...</td>\n",
       "      <td>tensor([[ 1.9386e-01,  1.0891e-01,  1.7538e-01...</td>\n",
       "      <td>tensor([[ 3.7038e-01,  3.1744e-01, -1.2233e-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>tensor([[-3.0913e-02, -1.4959e-01, -3.3085e-01...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "      <td>tensor([[-1.8501e-02, -1.9456e-01,  3.0173e-02...</td>\n",
       "      <td>tensor([[ 1.0203e-01,  7.4040e-03, -3.2376e-01...</td>\n",
       "      <td>tensor([[ 3.0898e-01,  2.0793e-01, -4.3984e-01...</td>\n",
       "      <td>tensor([[-3.3589e-01,  9.8876e-02, -1.0965e-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>tensor([[-7.7441e-02,  1.9496e-01, -3.8529e-01...</td>\n",
       "      <td>multi-sarcasm</td>\n",
       "      <td>tensor([[ 9.5810e-02, -2.5979e-01,  6.2205e-02...</td>\n",
       "      <td>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...</td>\n",
       "      <td>tensor([[-1.4243e-03,  1.6136e-01, -3.6162e-01...</td>\n",
       "      <td>tensor([[ 3.1547e-01, -1.3707e-01, -2.7851e-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>10800</td>\n",
       "      <td>10800</td>\n",
       "      <td>10800</td>\n",
       "      <td>tensor([[ 1.9143e-01, -8.0573e-02, -5.2628e-01...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "      <td>tensor([[-3.1771e-01,  1.8235e-01,  9.2051e-02...</td>\n",
       "      <td>tensor([[-1.3304e-01, -1.4935e-01, -2.7408e-01...</td>\n",
       "      <td>tensor([[ 2.8199e-01, -1.5914e-01, -1.4282e-01...</td>\n",
       "      <td>tensor([[-6.3018e-01, -1.9213e-01, -2.0101e-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10801</th>\n",
       "      <td>10801</td>\n",
       "      <td>10801</td>\n",
       "      <td>10801</td>\n",
       "      <td>tensor([[-2.2732e-01, -9.4946e-02, -6.8110e-02...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "      <td>tensor([[-2.5732e-01, -8.0080e-02, -2.0398e-01...</td>\n",
       "      <td>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...</td>\n",
       "      <td>tensor([[ 3.7033e-01,  1.1122e-01,  1.4711e-02...</td>\n",
       "      <td>tensor([[-0.0269,  0.5335, -0.1097,  0.0121, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>10802</td>\n",
       "      <td>10802</td>\n",
       "      <td>10802</td>\n",
       "      <td>tensor([[ 1.4880e-01, -2.0419e-01,  2.2676e-01...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "      <td>tensor([[-3.1760e-01,  5.6511e-01, -8.7909e-02...</td>\n",
       "      <td>tensor([[-6.1615e-02,  1.7048e-01, -1.7485e-01...</td>\n",
       "      <td>tensor([[-6.7110e-02,  1.9595e-01, -3.0825e-01...</td>\n",
       "      <td>tensor([[-6.8516e-01, -2.8459e-01, -2.3101e-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10803</th>\n",
       "      <td>10803</td>\n",
       "      <td>10803</td>\n",
       "      <td>10803</td>\n",
       "      <td>tensor([[ 1.6488e-01,  2.4950e-01, -2.8213e-02...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "      <td>tensor([[-3.9500e-01,  5.8485e-01, -2.0037e-02...</td>\n",
       "      <td>tensor([[ 4.2279e-01,  1.5298e-01, -1.2523e-01...</td>\n",
       "      <td>tensor([[ 2.8199e-01, -1.5914e-01, -1.4282e-01...</td>\n",
       "      <td>tensor([[-6.8516e-01, -2.8459e-01, -2.3101e-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10804</th>\n",
       "      <td>10804</td>\n",
       "      <td>10804</td>\n",
       "      <td>10804</td>\n",
       "      <td>tensor([[ 4.7611e-02,  1.0246e-01, -1.4754e-01...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "      <td>tensor([[-2.1180e-01,  2.6338e-02, -4.7125e-01...</td>\n",
       "      <td>tensor([[ 4.5899e-01, -4.0766e-01, -1.7532e-01...</td>\n",
       "      <td>tensor([[ 2.4169e-01,  7.7020e-02, -1.2376e-02...</td>\n",
       "      <td>tensor([[-1.7788e-01,  1.7226e-02, -2.6140e-01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10805 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0     id  \\\n",
       "0                 0           0      0   \n",
       "1                 1           1      1   \n",
       "2                 2           2      2   \n",
       "3                 3           3      3   \n",
       "4                 4           4      4   \n",
       "...             ...         ...    ...   \n",
       "10800         10800       10800  10800   \n",
       "10801         10801       10801  10801   \n",
       "10802         10802       10802  10802   \n",
       "10803         10803       10803  10803   \n",
       "10804         10804       10804  10804   \n",
       "\n",
       "                                        caption_features        sarcasm  \\\n",
       "0      tensor([[ 8.7540e-02, -8.4067e-03, -5.7961e-01...  multi-sarcasm   \n",
       "1      tensor([[-1.4093e-01,  2.6814e-01,  6.6393e-02...    not-sarcasm   \n",
       "2      tensor([[ 2.9297e-02,  7.8513e-02,  1.9056e-02...  multi-sarcasm   \n",
       "3      tensor([[-3.0913e-02, -1.4959e-01, -3.3085e-01...    not-sarcasm   \n",
       "4      tensor([[-7.7441e-02,  1.9496e-01, -3.8529e-01...  multi-sarcasm   \n",
       "...                                                  ...            ...   \n",
       "10800  tensor([[ 1.9143e-01, -8.0573e-02, -5.2628e-01...    not-sarcasm   \n",
       "10801  tensor([[-2.2732e-01, -9.4946e-02, -6.8110e-02...    not-sarcasm   \n",
       "10802  tensor([[ 1.4880e-01, -2.0419e-01,  2.2676e-01...    not-sarcasm   \n",
       "10803  tensor([[ 1.6488e-01,  2.4950e-01, -2.8213e-02...    not-sarcasm   \n",
       "10804  tensor([[ 4.7611e-02,  1.0246e-01, -1.4754e-01...    not-sarcasm   \n",
       "\n",
       "                                            image_vector  \\\n",
       "0      tensor([[ 7.2288e-01,  1.0506e-02, -7.5529e-02...   \n",
       "1      tensor([[ 2.4603e-02,  7.0014e-01,  2.0763e-01...   \n",
       "2      tensor([[ 2.8290e-01,  1.8930e-01, -7.7651e-02...   \n",
       "3      tensor([[-1.8501e-02, -1.9456e-01,  3.0173e-02...   \n",
       "4      tensor([[ 9.5810e-02, -2.5979e-01,  6.2205e-02...   \n",
       "...                                                  ...   \n",
       "10800  tensor([[-3.1771e-01,  1.8235e-01,  9.2051e-02...   \n",
       "10801  tensor([[-2.5732e-01, -8.0080e-02, -2.0398e-01...   \n",
       "10802  tensor([[-3.1760e-01,  5.6511e-01, -8.7909e-02...   \n",
       "10803  tensor([[-3.9500e-01,  5.8485e-01, -2.0037e-02...   \n",
       "10804  tensor([[-2.1180e-01,  2.6338e-02, -4.7125e-01...   \n",
       "\n",
       "                                              image_text  \\\n",
       "0      tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...   \n",
       "1      tensor([[-2.9716e-01,  3.0908e-01, -1.9151e-01...   \n",
       "2      tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...   \n",
       "3      tensor([[ 1.0203e-01,  7.4040e-03, -3.2376e-01...   \n",
       "4      tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...   \n",
       "...                                                  ...   \n",
       "10800  tensor([[-1.3304e-01, -1.4935e-01, -2.7408e-01...   \n",
       "10801  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0...   \n",
       "10802  tensor([[-6.1615e-02,  1.7048e-01, -1.7485e-01...   \n",
       "10803  tensor([[ 4.2279e-01,  1.5298e-01, -1.2523e-01...   \n",
       "10804  tensor([[ 4.5899e-01, -4.0766e-01, -1.7532e-01...   \n",
       "\n",
       "                                                  object  \\\n",
       "0      tensor([[ 2.4169e-01,  7.7020e-02, -1.2376e-02...   \n",
       "1      tensor([[ 2.8199e-01, -1.5914e-01, -1.4282e-01...   \n",
       "2      tensor([[ 1.9386e-01,  1.0891e-01,  1.7538e-01...   \n",
       "3      tensor([[ 3.0898e-01,  2.0793e-01, -4.3984e-01...   \n",
       "4      tensor([[-1.4243e-03,  1.6136e-01, -3.6162e-01...   \n",
       "...                                                  ...   \n",
       "10800  tensor([[ 2.8199e-01, -1.5914e-01, -1.4282e-01...   \n",
       "10801  tensor([[ 3.7033e-01,  1.1122e-01,  1.4711e-02...   \n",
       "10802  tensor([[-6.7110e-02,  1.9595e-01, -3.0825e-01...   \n",
       "10803  tensor([[ 2.8199e-01, -1.5914e-01, -1.4282e-01...   \n",
       "10804  tensor([[ 2.4169e-01,  7.7020e-02, -1.2376e-02...   \n",
       "\n",
       "                                             description  \n",
       "0      tensor([[ 1.4588e-01, -1.4534e-02, -3.4795e-01...  \n",
       "1      tensor([[-5.6289e-02,  4.7537e-01, -6.9459e-02...  \n",
       "2      tensor([[ 3.7038e-01,  3.1744e-01, -1.2233e-01...  \n",
       "3      tensor([[-3.3589e-01,  9.8876e-02, -1.0965e-01...  \n",
       "4      tensor([[ 3.1547e-01, -1.3707e-01, -2.7851e-01...  \n",
       "...                                                  ...  \n",
       "10800  tensor([[-6.3018e-01, -1.9213e-01, -2.0101e-01...  \n",
       "10801  tensor([[-0.0269,  0.5335, -0.1097,  0.0121, -...  \n",
       "10802  tensor([[-6.8516e-01, -2.8459e-01, -2.3101e-01...  \n",
       "10803  tensor([[-6.8516e-01, -2.8459e-01, -2.3101e-01...  \n",
       "10804  tensor([[-1.7788e-01,  1.7226e-02, -2.6140e-01...  \n",
       "\n",
       "[10805 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(folder + 'clip_data\\\\' + 'final_train_eng_combined.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public = pd.read_csv(folder + 'clip_data\\\\' + 'final_dev_public_eng_combined.csv')\n",
    "df_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tensor_array(string_data):\n",
    "    # Chuyển đổi chuỗi chứa tensor thành mảng NumPy\n",
    "    # Loại bỏ 'tensor(' và ')', và sử dụng ast.literal_eval để chuyển đổi\n",
    "    array_data = ast.literal_eval(string_data.replace('tensor(', '').replace(')', ''))\n",
    "    return np.array(array_data)\n",
    "\n",
    "def tranforms(data):\n",
    "    # Chuyển đổi cột image và text thành NumPy arrays\n",
    "    data['image_vector'] = data['image_vector'].apply(string_to_tensor_array)\n",
    "    data['caption_features'] = data['caption_features'].apply(string_to_tensor_array)\n",
    "    data['image_text'] = data['image_text'].apply(string_to_tensor_array)\n",
    "\n",
    "    # Chuyển đổi thành NumPy arrays\n",
    "    res_images = np.array(data['image_vector'].tolist())\n",
    "    res_texts = np.array(data['caption_features'].tolist())\n",
    "    res_image_text = np.array(data['image_text'].tolist())\n",
    "\n",
    "    return res_images, res_texts, res_image_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_images, X_train_texts, X_train_images_text = tranforms(df_train)\n",
    "y_train = df_train['sarcasm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_images, X_dev_texts, X_dev_images_text = tranforms(df_public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các giá trị độc nhất trong y_train:\n",
      "['multi-sarcasm' 'not-sarcasm' 'image-sarcasm' 'text-sarcasm']\n"
     ]
    }
   ],
   "source": [
    "# In ra các giá trị độc nhất trong y_train\n",
    "print(\"Các giá trị độc nhất trong y_train:\")\n",
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'not-sarcasm': 0,\n",
    "    'multi-sarcasm': 1,\n",
    "    'image-sarcasm': 2,\n",
    "    'text-sarcasm': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (10805, 1, 768)\n",
      "Shape of training texts: (10805, 1, 768)\n",
      "Shape of training images_text: (10805, 1, 768)\n",
      "Shape of training labels: (10805,)\n"
     ]
    }
   ],
   "source": [
    "# Xuất kích thước của các numpy arrays\n",
    "print(\"Shape of training images:\", X_train_images.shape)\n",
    "print(\"Shape of training texts:\", X_train_texts.shape)\n",
    "print(\"Shape of training images_text:\", X_train_images_text.shape)\n",
    "print(\"Shape of training labels:\", y_train.shape)\n",
    "# print(\"Shape of validation images:\", val_x_images.shape)\n",
    "# print(\"Shape of validation texts:\", val_x_texts.shape)\n",
    "# print(\"Shape of validation images_text:\", val_x_images_text.shape)\n",
    "# print(\"Shape of validation texts:\", val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __MÔ HÌNH__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Epoch 1/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step   - accuracy: 0.5925 \n",
      " - val_f1_macro: 0.4067\n",
      "Model saved with val_f1_macro: 0.4067\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 28ms/step - accuracy: 0.5927 - loss: 0.9454 - val_accuracy: 0.6553 - val_loss: 0.7626 - val_f1_macro: 0.4067\n",
      "Epoch 2/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.79\n",
      " - val_f1_macro: 0.4148\n",
      "Model saved with val_f1_macro: 0.4148\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 30ms/step - accuracy: 0.7936 - loss: 0.5200 - val_accuracy: 0.6432 - val_loss: 0.7830 - val_f1_macro: 0.4148\n",
      "Epoch 3/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.8470 -\n",
      " - val_f1_macro: 0.4141\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8470 - loss: 0.3956 - val_accuracy: 0.6548 - val_loss: 0.7672 - val_f1_macro: 0.4141\n",
      "Epoch 4/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8659 -\n",
      " - val_f1_macro: 0.4229\n",
      "Model saved with val_f1_macro: 0.4229\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.8659 - loss: 0.3399 - val_accuracy: 0.6529 - val_loss: 0.7964 - val_f1_macro: 0.4229\n",
      "Epoch 5/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step p - accuracy\n",
      " - val_f1_macro: 0.4197\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.8810 - loss: 0.2992 - val_accuracy: 0.6497 - val_loss: 0.8261 - val_f1_macro: 0.4197\n",
      "Epoch 6/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step  p - accuracy: 0.894\n",
      " - val_f1_macro: 0.4204\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - accuracy: 0.8945 - loss: 0.2655 - val_accuracy: 0.6705 - val_loss: 0.8283 - val_f1_macro: 0.4204\n",
      "Epoch 7/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9010 -\n",
      " - val_f1_macro: 0.4218\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 21ms/step - accuracy: 0.9010 - loss: 0.2445 - val_accuracy: 0.6696 - val_loss: 0.8613 - val_f1_macro: 0.4218\n",
      "Epoch 8/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9115 -\n",
      " - val_f1_macro: 0.4084\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9115 - loss: 0.2194 - val_accuracy: 0.6622 - val_loss: 0.8615 - val_f1_macro: 0.4084\n",
      "Epoch 9/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9183 - l\n",
      " - val_f1_macro: 0.4203\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9183 - loss: 0.2059 - val_accuracy: 0.6770 - val_loss: 0.8905 - val_f1_macro: 0.4203\n",
      "Epoch 10/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9223 -\n",
      " - val_f1_macro: 0.4225\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9223 - loss: 0.1949 - val_accuracy: 0.6728 - val_loss: 0.9165 - val_f1_macro: 0.4225\n",
      "Epoch 11/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9277 -\n",
      " - val_f1_macro: 0.4187\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9277 - loss: 0.1790 - val_accuracy: 0.6677 - val_loss: 0.9597 - val_f1_macro: 0.4187\n",
      "Epoch 12/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9310\n",
      " - val_f1_macro: 0.4288\n",
      "Model saved with val_f1_macro: 0.4288\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9310 - loss: 0.1749 - val_accuracy: 0.6673 - val_loss: 1.0008 - val_f1_macro: 0.4288\n",
      "Epoch 13/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9383\n",
      " - val_f1_macro: 0.4093\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9383 - loss: 0.1584 - val_accuracy: 0.6571 - val_loss: 1.1147 - val_f1_macro: 0.4093\n",
      "Epoch 14/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.94\n",
      " - val_f1_macro: 0.4137\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9412 - loss: 0.1473 - val_accuracy: 0.6664 - val_loss: 1.0739 - val_f1_macro: 0.4137\n",
      "Epoch 15/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.94\n",
      " - val_f1_macro: 0.4183\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - accuracy: 0.9410 - loss: 0.1480 - val_accuracy: 0.6622 - val_loss: 1.0848 - val_f1_macro: 0.4183\n",
      "Epoch 16/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9497 - l\n",
      " - val_f1_macro: 0.4176\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9497 - loss: 0.1328 - val_accuracy: 0.6724 - val_loss: 1.1034 - val_f1_macro: 0.4176\n",
      "Epoch 17/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9486\n",
      " - val_f1_macro: 0.4218\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9486 - loss: 0.1289 - val_accuracy: 0.6747 - val_loss: 1.1491 - val_f1_macro: 0.4218\n",
      "Epoch 18/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9507\n",
      " - val_f1_macro: 0.3997\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9507 - loss: 0.1249 - val_accuracy: 0.6668 - val_loss: 1.1699 - val_f1_macro: 0.3997\n",
      "Epoch 19/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step  p - accuracy: 0.9\n",
      " - val_f1_macro: 0.3966\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9589 - loss: 0.1071 - val_accuracy: 0.6566 - val_loss: 1.2564 - val_f1_macro: 0.3966\n",
      "Epoch 20/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9604\n",
      " - val_f1_macro: 0.4089\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9604 - loss: 0.1030 - val_accuracy: 0.6747 - val_loss: 1.2690 - val_f1_macro: 0.4089\n",
      "Epoch 21/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9600 -\n",
      " - val_f1_macro: 0.4002\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9600 - loss: 0.1062 - val_accuracy: 0.6627 - val_loss: 1.3083 - val_f1_macro: 0.4002\n",
      "Epoch 22/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9609\n",
      " - val_f1_macro: 0.4221\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9608 - loss: 0.0993 - val_accuracy: 0.6603 - val_loss: 1.2961 - val_f1_macro: 0.4221\n",
      "Epoch 23/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9636\n",
      " - val_f1_macro: 0.4174\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step - accuracy: 0.9636 - loss: 0.0988 - val_accuracy: 0.6636 - val_loss: 1.3015 - val_f1_macro: 0.4174\n",
      "Epoch 24/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step ep - accuracy: 0.96\n",
      " - val_f1_macro: 0.4123\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9656 - loss: 0.0940 - val_accuracy: 0.6636 - val_loss: 1.4170 - val_f1_macro: 0.4123\n",
      "Epoch 25/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9652\n",
      " - val_f1_macro: 0.4127\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9652 - loss: 0.0894 - val_accuracy: 0.6645 - val_loss: 1.3829 - val_f1_macro: 0.4127\n",
      "Epoch 26/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9669\n",
      " - val_f1_macro: 0.4150\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9669 - loss: 0.0852 - val_accuracy: 0.6714 - val_loss: 1.4418 - val_f1_macro: 0.4150\n",
      "Epoch 27/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.4234\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9701 - loss: 0.0828 - val_accuracy: 0.6719 - val_loss: 1.4771 - val_f1_macro: 0.4234\n",
      "Epoch 28/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9669\n",
      " - val_f1_macro: 0.4155\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9669 - loss: 0.0815 - val_accuracy: 0.6701 - val_loss: 1.4774 - val_f1_macro: 0.4155\n",
      "Epoch 29/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step ep - accuracy: 0.96\n",
      " - val_f1_macro: 0.4078\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9671 - loss: 0.0827 - val_accuracy: 0.6733 - val_loss: 1.5209 - val_f1_macro: 0.4078\n",
      "Epoch 30/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.9706\n",
      " - val_f1_macro: 0.4221\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9706 - loss: 0.0761 - val_accuracy: 0.6701 - val_loss: 1.5418 - val_f1_macro: 0.4221\n",
      "Epoch 31/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4124\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9732 - loss: 0.0721 - val_accuracy: 0.6682 - val_loss: 1.5941 - val_f1_macro: 0.4124\n",
      "Epoch 32/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4066\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9737 - loss: 0.0723 - val_accuracy: 0.6631 - val_loss: 1.6558 - val_f1_macro: 0.4066\n",
      "Epoch 33/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4050\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.9760 - loss: 0.0653 - val_accuracy: 0.6668 - val_loss: 1.7003 - val_f1_macro: 0.4050\n",
      "Epoch 34/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step p - accuracy: 0\n",
      " - val_f1_macro: 0.4250\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 30ms/step - accuracy: 0.9726 - loss: 0.0723 - val_accuracy: 0.6798 - val_loss: 1.6482 - val_f1_macro: 0.4250\n",
      "Epoch 35/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step ep - accuracy: 0\n",
      " - val_f1_macro: 0.4201\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9778 - loss: 0.0594 - val_accuracy: 0.6779 - val_loss: 1.7360 - val_f1_macro: 0.4201\n",
      "Epoch 36/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step    - accuracy: 0.9791 \n",
      " - val_f1_macro: 0.4158\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 38ms/step - accuracy: 0.9791 - loss: 0.0591 - val_accuracy: 0.6724 - val_loss: 1.7032 - val_f1_macro: 0.4158\n",
      "Epoch 37/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: \n",
      " - val_f1_macro: 0.4091\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9748 - loss: 0.0660 - val_accuracy: 0.6714 - val_loss: 1.7404 - val_f1_macro: 0.4091\n",
      "Epoch 38/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4118\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.9758 - loss: 0.0667 - val_accuracy: 0.6714 - val_loss: 1.7450 - val_f1_macro: 0.4118\n",
      "Epoch 39/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step p - accuracy: 0.978\n",
      " - val_f1_macro: 0.4013\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9784 - loss: 0.0596 - val_accuracy: 0.6659 - val_loss: 1.7530 - val_f1_macro: 0.4013\n",
      "Epoch 40/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step ep - accuracy: 0.9791 - l\n",
      " - val_f1_macro: 0.4082\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9791 - loss: 0.0586 - val_accuracy: 0.6668 - val_loss: 1.8104 - val_f1_macro: 0.4082\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "Best model val_f1_macro for fold 1: 42.88%\n",
      "Training fold 2...\n",
      "Epoch 1/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step   - accuracy: 0.596\n",
      " - val_f1_macro: 0.3951\n",
      "Model saved with val_f1_macro: 0.3951\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 31ms/step - accuracy: 0.5969 - loss: 0.9517 - val_accuracy: 0.6118 - val_loss: 0.8531 - val_f1_macro: 0.3951\n",
      "Epoch 2/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.7955 -\n",
      " - val_f1_macro: 0.4034\n",
      "Model saved with val_f1_macro: 0.4034\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.7956 - loss: 0.5164 - val_accuracy: 0.6307 - val_loss: 0.8051 - val_f1_macro: 0.4034\n",
      "Epoch 3/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8423 -\n",
      " - val_f1_macro: 0.4274\n",
      "Model saved with val_f1_macro: 0.4274\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.8423 - loss: 0.4002 - val_accuracy: 0.6603 - val_loss: 0.7725 - val_f1_macro: 0.4274\n",
      "Epoch 4/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step   - accuracy: 0.8614 - \n",
      " - val_f1_macro: 0.3958\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 30ms/step - accuracy: 0.8614 - loss: 0.3409 - val_accuracy: 0.6580 - val_loss: 0.7941 - val_f1_macro: 0.3958\n",
      "Epoch 5/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step ep - accuracy: 0.8823\n",
      " - val_f1_macro: 0.4343\n",
      "Model saved with val_f1_macro: 0.4343\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.8823 - loss: 0.2904 - val_accuracy: 0.6724 - val_loss: 0.7712 - val_f1_macro: 0.4343\n",
      "Epoch 6/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.88\n",
      " - val_f1_macro: 0.3974\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.8885 - loss: 0.2726 - val_accuracy: 0.6752 - val_loss: 0.7889 - val_f1_macro: 0.3974\n",
      "Epoch 7/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9046 -\n",
      " - val_f1_macro: 0.3948\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 22ms/step - accuracy: 0.9046 - loss: 0.2424 - val_accuracy: 0.6627 - val_loss: 0.8347 - val_f1_macro: 0.3948\n",
      "Epoch 8/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step   - accuracy: 0.9072\n",
      " - val_f1_macro: 0.3996\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.9072 - loss: 0.2308 - val_accuracy: 0.6779 - val_loss: 0.8207 - val_f1_macro: 0.3996\n",
      "Epoch 9/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.91\n",
      " - val_f1_macro: 0.3921\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9197 - loss: 0.2028 - val_accuracy: 0.6765 - val_loss: 0.8468 - val_f1_macro: 0.3921\n",
      "Epoch 10/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step ep - accuracy: 0.9233\n",
      " - val_f1_macro: 0.3835\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 31ms/step - accuracy: 0.9233 - loss: 0.1953 - val_accuracy: 0.6659 - val_loss: 0.9171 - val_f1_macro: 0.3835\n",
      "Epoch 11/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9303\n",
      " - val_f1_macro: 0.4083\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9302 - loss: 0.1791 - val_accuracy: 0.6932 - val_loss: 0.9085 - val_f1_macro: 0.4083\n",
      "Epoch 12/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/stepep - accuracy: 0.93\n",
      " - val_f1_macro: 0.3885\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 38ms/step - accuracy: 0.9354 - loss: 0.1662 - val_accuracy: 0.6747 - val_loss: 0.9320 - val_f1_macro: 0.3885\n",
      "Epoch 13/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9394\n",
      " - val_f1_macro: 0.3970\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 29ms/step - accuracy: 0.9394 - loss: 0.1586 - val_accuracy: 0.6830 - val_loss: 0.9198 - val_f1_macro: 0.3970\n",
      "Epoch 14/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step    - accuracy: 0.\n",
      " - val_f1_macro: 0.3956\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9442 - loss: 0.1457 - val_accuracy: 0.6742 - val_loss: 0.9756 - val_f1_macro: 0.3956\n",
      "Epoch 15/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.94\n",
      " - val_f1_macro: 0.4033\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9466 - loss: 0.1395 - val_accuracy: 0.6863 - val_loss: 0.9944 - val_f1_macro: 0.4033\n",
      "Epoch 16/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9518\n",
      " - val_f1_macro: 0.3928\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.9518 - loss: 0.1262 - val_accuracy: 0.6826 - val_loss: 1.0471 - val_f1_macro: 0.3928\n",
      "Epoch 17/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step ep - accuracy\n",
      " - val_f1_macro: 0.3948\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.9516 - loss: 0.1229 - val_accuracy: 0.6742 - val_loss: 1.0734 - val_f1_macro: 0.3948\n",
      "Epoch 18/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9519 -\n",
      " - val_f1_macro: 0.3890\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9519 - loss: 0.1231 - val_accuracy: 0.6724 - val_loss: 1.1250 - val_f1_macro: 0.3890\n",
      "Epoch 19/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9541 -\n",
      " - val_f1_macro: 0.3936\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.9541 - loss: 0.1175 - val_accuracy: 0.6789 - val_loss: 1.1309 - val_f1_macro: 0.3936\n",
      "Epoch 20/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9592\n",
      " - val_f1_macro: 0.3901\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9592 - loss: 0.1050 - val_accuracy: 0.6701 - val_loss: 1.1209 - val_f1_macro: 0.3901\n",
      "Epoch 21/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step ep - accuracy: 0.9598 -\n",
      " - val_f1_macro: 0.3768\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9598 - loss: 0.1035 - val_accuracy: 0.6696 - val_loss: 1.1755 - val_f1_macro: 0.3768\n",
      "Epoch 22/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.9620\n",
      " - val_f1_macro: 0.3901\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9620 - loss: 0.1009 - val_accuracy: 0.6761 - val_loss: 1.2071 - val_f1_macro: 0.3901\n",
      "Epoch 23/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/stepep - accuracy: 0.963\n",
      " - val_f1_macro: 0.3973\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9635 - loss: 0.0949 - val_accuracy: 0.6849 - val_loss: 1.2186 - val_f1_macro: 0.3973\n",
      "Epoch 24/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.968\n",
      " - val_f1_macro: 0.3977\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9684 - loss: 0.0869 - val_accuracy: 0.6789 - val_loss: 1.2485 - val_f1_macro: 0.3977\n",
      "Epoch 25/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9668 -\n",
      " - val_f1_macro: 0.3853\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9668 - loss: 0.0883 - val_accuracy: 0.6682 - val_loss: 1.3276 - val_f1_macro: 0.3853\n",
      "Epoch 26/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9660\n",
      " - val_f1_macro: 0.3971\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - accuracy: 0.9660 - loss: 0.0894 - val_accuracy: 0.6807 - val_loss: 1.3258 - val_f1_macro: 0.3971\n",
      "Epoch 27/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9725\n",
      " - val_f1_macro: 0.3945\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - accuracy: 0.9725 - loss: 0.0760 - val_accuracy: 0.6807 - val_loss: 1.3333 - val_f1_macro: 0.3945\n",
      "Epoch 28/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9650 -\n",
      " - val_f1_macro: 0.3940\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9650 - loss: 0.0868 - val_accuracy: 0.6719 - val_loss: 1.4422 - val_f1_macro: 0.3940\n",
      "Epoch 29/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9686 -\n",
      " - val_f1_macro: 0.3906\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9686 - loss: 0.0836 - val_accuracy: 0.6752 - val_loss: 1.4252 - val_f1_macro: 0.3906\n",
      "Epoch 30/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9721\n",
      " - val_f1_macro: 0.4021\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9721 - loss: 0.0746 - val_accuracy: 0.6798 - val_loss: 1.3903 - val_f1_macro: 0.4021\n",
      "Epoch 31/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9736 -\n",
      " - val_f1_macro: 0.3953\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9736 - loss: 0.0692 - val_accuracy: 0.6742 - val_loss: 1.5065 - val_f1_macro: 0.3953\n",
      "Epoch 32/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9748\n",
      " - val_f1_macro: 0.3895\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9748 - loss: 0.0673 - val_accuracy: 0.6705 - val_loss: 1.4152 - val_f1_macro: 0.3895\n",
      "Epoch 33/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9733 -\n",
      " - val_f1_macro: 0.3894\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9733 - loss: 0.0673 - val_accuracy: 0.6784 - val_loss: 1.5755 - val_f1_macro: 0.3894\n",
      "Epoch 34/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.3869\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9736 - loss: 0.0710 - val_accuracy: 0.6733 - val_loss: 1.4672 - val_f1_macro: 0.3869\n",
      "Epoch 35/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9772 -\n",
      " - val_f1_macro: 0.3869\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9772 - loss: 0.0651 - val_accuracy: 0.6701 - val_loss: 1.6070 - val_f1_macro: 0.3869\n",
      "Epoch 36/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9755 -\n",
      " - val_f1_macro: 0.3901\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9755 - loss: 0.0628 - val_accuracy: 0.6779 - val_loss: 1.5847 - val_f1_macro: 0.3901\n",
      "Epoch 37/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9764 -\n",
      " - val_f1_macro: 0.3896\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9764 - loss: 0.0635 - val_accuracy: 0.6691 - val_loss: 1.6041 - val_f1_macro: 0.3896\n",
      "Epoch 38/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9771\n",
      " - val_f1_macro: 0.3755\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - accuracy: 0.9771 - loss: 0.0581 - val_accuracy: 0.6576 - val_loss: 1.6018 - val_f1_macro: 0.3755\n",
      "Epoch 39/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9778 -\n",
      " - val_f1_macro: 0.3785\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - accuracy: 0.9778 - loss: 0.0617 - val_accuracy: 0.6645 - val_loss: 1.6439 - val_f1_macro: 0.3785\n",
      "Epoch 40/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9795 -\n",
      " - val_f1_macro: 0.3781\n",
      "\u001b[1m608/608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9795 - loss: 0.0568 - val_accuracy: 0.6580 - val_loss: 1.6843 - val_f1_macro: 0.3781\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "Best model val_f1_macro for fold 2: 43.43%\n",
      "Training fold 3...\n",
      "Epoch 1/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/stepep - accuracy: 0.5860\n",
      " - val_f1_macro: 0.4544\n",
      "Model saved with val_f1_macro: 0.4544\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 44ms/step - accuracy: 0.5862 - loss: 0.9767 - val_accuracy: 0.6645 - val_loss: 0.7702 - val_f1_macro: 0.4544\n",
      "Epoch 2/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.7956 -\n",
      " - val_f1_macro: 0.4536\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.7956 - loss: 0.5105 - val_accuracy: 0.6354 - val_loss: 0.8119 - val_f1_macro: 0.4536\n",
      "Epoch 3/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/stepep - accuracy: 0.8\n",
      " - val_f1_macro: 0.4455\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8382 - loss: 0.4051 - val_accuracy: 0.6492 - val_loss: 0.8005 - val_f1_macro: 0.4455\n",
      "Epoch 4/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.8647\n",
      " - val_f1_macro: 0.4493\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8648 - loss: 0.3386 - val_accuracy: 0.6391 - val_loss: 0.8684 - val_f1_macro: 0.4493\n",
      "Epoch 5/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step ep - accuracy: 0.87\n",
      " - val_f1_macro: 0.4763\n",
      "Model saved with val_f1_macro: 0.4763\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.8784 - loss: 0.3015 - val_accuracy: 0.6691 - val_loss: 0.8230 - val_f1_macro: 0.4763\n",
      "Epoch 6/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.89\n",
      " - val_f1_macro: 0.4759\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.8939 - loss: 0.2641 - val_accuracy: 0.6687 - val_loss: 0.8689 - val_f1_macro: 0.4759\n",
      "Epoch 7/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8984 -\n",
      " - val_f1_macro: 0.4612\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8984 - loss: 0.2498 - val_accuracy: 0.6733 - val_loss: 0.8891 - val_f1_macro: 0.4612\n",
      "Epoch 8/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9070 -\n",
      " - val_f1_macro: 0.4781\n",
      "Model saved with val_f1_macro: 0.4781\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9070 - loss: 0.2265 - val_accuracy: 0.6636 - val_loss: 0.9466 - val_f1_macro: 0.4781\n",
      "Epoch 9/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9210 -\n",
      " - val_f1_macro: 0.4423\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9210 - loss: 0.2059 - val_accuracy: 0.6789 - val_loss: 0.9170 - val_f1_macro: 0.4423\n",
      "Epoch 10/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/stepep - accuracy: 0.925\n",
      " - val_f1_macro: 0.4602\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9254 - loss: 0.1958 - val_accuracy: 0.6696 - val_loss: 0.9582 - val_f1_macro: 0.4602\n",
      "Epoch 11/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9318\n",
      " - val_f1_macro: 0.4805\n",
      "Model saved with val_f1_macro: 0.4805\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 27ms/step - accuracy: 0.9318 - loss: 0.1756 - val_accuracy: 0.6844 - val_loss: 0.9911 - val_f1_macro: 0.4805\n",
      "Epoch 12/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9338 -\n",
      " - val_f1_macro: 0.4455\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9338 - loss: 0.1660 - val_accuracy: 0.6617 - val_loss: 1.0524 - val_f1_macro: 0.4455\n",
      "Epoch 13/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9393 -\n",
      " - val_f1_macro: 0.4540\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9393 - loss: 0.1576 - val_accuracy: 0.6617 - val_loss: 1.0856 - val_f1_macro: 0.4540\n",
      "Epoch 14/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.4529\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 0.9437 - loss: 0.1452 - val_accuracy: 0.6738 - val_loss: 1.1055 - val_f1_macro: 0.4529\n",
      "Epoch 15/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9455\n",
      " - val_f1_macro: 0.4564\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9455 - loss: 0.1388 - val_accuracy: 0.6673 - val_loss: 1.1355 - val_f1_macro: 0.4564\n",
      "Epoch 16/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9507 -\n",
      " - val_f1_macro: 0.4503\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9507 - loss: 0.1271 - val_accuracy: 0.6608 - val_loss: 1.1842 - val_f1_macro: 0.4503\n",
      "Epoch 17/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9493 -\n",
      " - val_f1_macro: 0.4746\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9493 - loss: 0.1295 - val_accuracy: 0.6770 - val_loss: 1.2367 - val_f1_macro: 0.4746\n",
      "Epoch 18/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9540 -\n",
      " - val_f1_macro: 0.4636\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9540 - loss: 0.1179 - val_accuracy: 0.6719 - val_loss: 1.2347 - val_f1_macro: 0.4636\n",
      "Epoch 19/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.9\n",
      " - val_f1_macro: 0.4900\n",
      "Model saved with val_f1_macro: 0.4900\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9580 - loss: 0.1091 - val_accuracy: 0.6691 - val_loss: 1.2788 - val_f1_macro: 0.4900\n",
      "Epoch 20/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step ep - accuracy\n",
      " - val_f1_macro: 0.4749\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9578 - loss: 0.1084 - val_accuracy: 0.6650 - val_loss: 1.3056 - val_f1_macro: 0.4749\n",
      "Epoch 21/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.95\n",
      " - val_f1_macro: 0.4601\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 30ms/step - accuracy: 0.9580 - loss: 0.1068 - val_accuracy: 0.6682 - val_loss: 1.3356 - val_f1_macro: 0.4601\n",
      "Epoch 22/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.4479\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.9614 - loss: 0.0979 - val_accuracy: 0.6594 - val_loss: 1.3518 - val_f1_macro: 0.4479\n",
      "Epoch 23/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step ep - accuracy: \n",
      " - val_f1_macro: 0.4489\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 37ms/step - accuracy: 0.9601 - loss: 0.0997 - val_accuracy: 0.6659 - val_loss: 1.4943 - val_f1_macro: 0.4489\n",
      "Epoch 24/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.96\n",
      " - val_f1_macro: 0.4541\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.9654 - loss: 0.0904 - val_accuracy: 0.6613 - val_loss: 1.4449 - val_f1_macro: 0.4541\n",
      "Epoch 25/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.96\n",
      " - val_f1_macro: 0.4257\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9635 - loss: 0.0948 - val_accuracy: 0.6645 - val_loss: 1.5506 - val_f1_macro: 0.4257\n",
      "Epoch 26/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9682\n",
      " - val_f1_macro: 0.4581\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9682 - loss: 0.0849 - val_accuracy: 0.6696 - val_loss: 1.4518 - val_f1_macro: 0.4581\n",
      "Epoch 27/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.4664\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 30ms/step - accuracy: 0.9697 - loss: 0.0818 - val_accuracy: 0.6728 - val_loss: 1.5495 - val_f1_macro: 0.4664\n",
      "Epoch 28/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/stepep - accuracy: 0.96\n",
      " - val_f1_macro: 0.4585\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 42ms/step - accuracy: 0.9663 - loss: 0.0830 - val_accuracy: 0.6603 - val_loss: 1.5716 - val_f1_macro: 0.4585\n",
      "Epoch 29/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9705 -\n",
      " - val_f1_macro: 0.4443\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9705 - loss: 0.0788 - val_accuracy: 0.6534 - val_loss: 1.6281 - val_f1_macro: 0.4443\n",
      "Epoch 30/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/stepep - accuracy: 0.9\n",
      " - val_f1_macro: 0.4466\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.9704 - loss: 0.0771 - val_accuracy: 0.6562 - val_loss: 1.7479 - val_f1_macro: 0.4466\n",
      "Epoch 31/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step    - accuracy: \n",
      " - val_f1_macro: 0.4639\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.9715 - loss: 0.0724 - val_accuracy: 0.6664 - val_loss: 1.6776 - val_f1_macro: 0.4639\n",
      "Epoch 32/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/stepep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4679\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9735 - loss: 0.0732 - val_accuracy: 0.6654 - val_loss: 1.6981 - val_f1_macro: 0.4679\n",
      "Epoch 33/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9755\n",
      " - val_f1_macro: 0.4693\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.9755 - loss: 0.0692 - val_accuracy: 0.6664 - val_loss: 1.7124 - val_f1_macro: 0.4693\n",
      "Epoch 34/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/stepep - accuracy: 0.975\n",
      " - val_f1_macro: 0.4500\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9757 - loss: 0.0647 - val_accuracy: 0.6571 - val_loss: 1.6870 - val_f1_macro: 0.4500\n",
      "Epoch 35/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step p - accuracy: 0.9748 -\n",
      " - val_f1_macro: 0.4533\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 40ms/step - accuracy: 0.9748 - loss: 0.0686 - val_accuracy: 0.6594 - val_loss: 1.6982 - val_f1_macro: 0.4533\n",
      "Epoch 36/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step p - accuracy: 0.9786\n",
      " - val_f1_macro: 0.4407\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 48ms/step - accuracy: 0.9786 - loss: 0.0594 - val_accuracy: 0.6590 - val_loss: 1.7591 - val_f1_macro: 0.4407\n",
      "Epoch 37/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/stepep - accuracy\n",
      " - val_f1_macro: 0.4280\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9758 - loss: 0.0650 - val_accuracy: 0.6659 - val_loss: 1.7750 - val_f1_macro: 0.4280\n",
      "Epoch 38/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step    - accuracy: \n",
      " - val_f1_macro: 0.4162\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9759 - loss: 0.0630 - val_accuracy: 0.6511 - val_loss: 1.8833 - val_f1_macro: 0.4162\n",
      "Epoch 39/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9752\n",
      " - val_f1_macro: 0.4425\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9752 - loss: 0.0666 - val_accuracy: 0.6557 - val_loss: 1.7472 - val_f1_macro: 0.4425\n",
      "Epoch 40/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4677\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9776 - loss: 0.0633 - val_accuracy: 0.6585 - val_loss: 1.9250 - val_f1_macro: 0.4677\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "Best model val_f1_macro for fold 3: 49.00%\n",
      "Training fold 4...\n",
      "Epoch 1/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step   - accuracy: 0.585\n",
      " - val_f1_macro: 0.4107\n",
      "Model saved with val_f1_macro: 0.4107\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 36ms/step - accuracy: 0.5856 - loss: 0.9651 - val_accuracy: 0.6011 - val_loss: 0.8448 - val_f1_macro: 0.4107\n",
      "Epoch 2/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.80\n",
      " - val_f1_macro: 0.4470\n",
      "Model saved with val_f1_macro: 0.4470\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.8008 - loss: 0.5080 - val_accuracy: 0.6242 - val_loss: 0.8252 - val_f1_macro: 0.4470\n",
      "Epoch 3/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8373 -\n",
      " - val_f1_macro: 0.4629\n",
      "Model saved with val_f1_macro: 0.4629\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.8373 - loss: 0.4087 - val_accuracy: 0.6613 - val_loss: 0.7618 - val_f1_macro: 0.4629\n",
      "Epoch 4/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8637 - l\n",
      " - val_f1_macro: 0.4579\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.8637 - loss: 0.3432 - val_accuracy: 0.6738 - val_loss: 0.7826 - val_f1_macro: 0.4579\n",
      "Epoch 5/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8737 - l\n",
      " - val_f1_macro: 0.4705\n",
      "Model saved with val_f1_macro: 0.4705\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.8737 - loss: 0.3041 - val_accuracy: 0.6691 - val_loss: 0.8020 - val_f1_macro: 0.4705\n",
      "Epoch 6/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8877 - l\n",
      " - val_f1_macro: 0.4514\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.8877 - loss: 0.2740 - val_accuracy: 0.6659 - val_loss: 0.8364 - val_f1_macro: 0.4514\n",
      "Epoch 7/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.4627\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9024 - loss: 0.2426 - val_accuracy: 0.6779 - val_loss: 0.8322 - val_f1_macro: 0.4627\n",
      "Epoch 8/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9112\n",
      " - val_f1_macro: 0.4690\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9112 - loss: 0.2220 - val_accuracy: 0.6668 - val_loss: 0.8757 - val_f1_macro: 0.4690\n",
      "Epoch 9/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.4663\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9146 - loss: 0.2147 - val_accuracy: 0.6807 - val_loss: 0.8899 - val_f1_macro: 0.4663\n",
      "Epoch 10/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9180 -\n",
      " - val_f1_macro: 0.4668\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9181 - loss: 0.1995 - val_accuracy: 0.6839 - val_loss: 0.9321 - val_f1_macro: 0.4668\n",
      "Epoch 11/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9271 -\n",
      " - val_f1_macro: 0.4713\n",
      "Model saved with val_f1_macro: 0.4713\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9271 - loss: 0.1778 - val_accuracy: 0.6761 - val_loss: 0.9696 - val_f1_macro: 0.4713\n",
      "Epoch 12/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.93\n",
      " - val_f1_macro: 0.4556\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9332 - loss: 0.1696 - val_accuracy: 0.6710 - val_loss: 0.9629 - val_f1_macro: 0.4556\n",
      "Epoch 13/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9367\n",
      " - val_f1_macro: 0.4586\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9367 - loss: 0.1577 - val_accuracy: 0.6784 - val_loss: 1.0406 - val_f1_macro: 0.4586\n",
      "Epoch 14/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9392 -\n",
      " - val_f1_macro: 0.4420\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9392 - loss: 0.1499 - val_accuracy: 0.6664 - val_loss: 1.0801 - val_f1_macro: 0.4420\n",
      "Epoch 15/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9450\n",
      " - val_f1_macro: 0.4401\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9449 - loss: 0.1412 - val_accuracy: 0.6668 - val_loss: 1.1593 - val_f1_macro: 0.4401\n",
      "Epoch 16/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9476 -\n",
      " - val_f1_macro: 0.4479\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9476 - loss: 0.1348 - val_accuracy: 0.6613 - val_loss: 1.0809 - val_f1_macro: 0.4479\n",
      "Epoch 17/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9504 -\n",
      " - val_f1_macro: 0.4426\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - accuracy: 0.9504 - loss: 0.1252 - val_accuracy: 0.6622 - val_loss: 1.1393 - val_f1_macro: 0.4426\n",
      "Epoch 18/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9538 -\n",
      " - val_f1_macro: 0.4392\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9538 - loss: 0.1217 - val_accuracy: 0.6599 - val_loss: 1.2148 - val_f1_macro: 0.4392\n",
      "Epoch 19/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9515 -\n",
      " - val_f1_macro: 0.4315\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9515 - loss: 0.1238 - val_accuracy: 0.6673 - val_loss: 1.2924 - val_f1_macro: 0.4315\n",
      "Epoch 20/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9528 -\n",
      " - val_f1_macro: 0.4543\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9528 - loss: 0.1168 - val_accuracy: 0.6738 - val_loss: 1.2684 - val_f1_macro: 0.4543\n",
      "Epoch 21/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.4534\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9618 - loss: 0.0989 - val_accuracy: 0.6728 - val_loss: 1.2787 - val_f1_macro: 0.4534\n",
      "Epoch 22/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9588 - l\n",
      " - val_f1_macro: 0.4424\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9588 - loss: 0.1035 - val_accuracy: 0.6761 - val_loss: 1.3289 - val_f1_macro: 0.4424\n",
      "Epoch 23/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9594 -\n",
      " - val_f1_macro: 0.4383\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9594 - loss: 0.1012 - val_accuracy: 0.6627 - val_loss: 1.2992 - val_f1_macro: 0.4383\n",
      "Epoch 24/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9675 - l\n",
      " - val_f1_macro: 0.4482\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9675 - loss: 0.0877 - val_accuracy: 0.6668 - val_loss: 1.3595 - val_f1_macro: 0.4482\n",
      "Epoch 25/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9641 -\n",
      " - val_f1_macro: 0.4273\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9641 - loss: 0.0935 - val_accuracy: 0.6576 - val_loss: 1.4403 - val_f1_macro: 0.4273\n",
      "Epoch 26/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9630 -\n",
      " - val_f1_macro: 0.4543\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9630 - loss: 0.0934 - val_accuracy: 0.6710 - val_loss: 1.3852 - val_f1_macro: 0.4543\n",
      "Epoch 27/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9714 - l\n",
      " - val_f1_macro: 0.4279\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9713 - loss: 0.0808 - val_accuracy: 0.6617 - val_loss: 1.5405 - val_f1_macro: 0.4279\n",
      "Epoch 28/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9665 -\n",
      " - val_f1_macro: 0.4481\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9665 - loss: 0.0855 - val_accuracy: 0.6659 - val_loss: 1.4523 - val_f1_macro: 0.4481\n",
      "Epoch 29/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9692\n",
      " - val_f1_macro: 0.4532\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - accuracy: 0.9692 - loss: 0.0787 - val_accuracy: 0.6682 - val_loss: 1.4738 - val_f1_macro: 0.4532\n",
      "Epoch 30/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9679 - l\n",
      " - val_f1_macro: 0.4387\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9679 - loss: 0.0807 - val_accuracy: 0.6571 - val_loss: 1.4810 - val_f1_macro: 0.4387\n",
      "Epoch 31/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9687 -\n",
      " - val_f1_macro: 0.4391\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9687 - loss: 0.0769 - val_accuracy: 0.6613 - val_loss: 1.4893 - val_f1_macro: 0.4391\n",
      "Epoch 32/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9736 -\n",
      " - val_f1_macro: 0.4505\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9736 - loss: 0.0731 - val_accuracy: 0.6650 - val_loss: 1.5293 - val_f1_macro: 0.4505\n",
      "Epoch 33/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9695 -\n",
      " - val_f1_macro: 0.4545\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9695 - loss: 0.0742 - val_accuracy: 0.6617 - val_loss: 1.5237 - val_f1_macro: 0.4545\n",
      "Epoch 34/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9722 -\n",
      " - val_f1_macro: 0.4448\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9722 - loss: 0.0708 - val_accuracy: 0.6677 - val_loss: 1.6917 - val_f1_macro: 0.4448\n",
      "Epoch 35/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9722 - l\n",
      " - val_f1_macro: 0.4374\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9722 - loss: 0.0717 - val_accuracy: 0.6617 - val_loss: 1.6477 - val_f1_macro: 0.4374\n",
      "Epoch 36/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9781 - \n",
      " - val_f1_macro: 0.4376\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9781 - loss: 0.0590 - val_accuracy: 0.6636 - val_loss: 1.6507 - val_f1_macro: 0.4376\n",
      "Epoch 37/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4353\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9741 - loss: 0.0685 - val_accuracy: 0.6664 - val_loss: 1.6312 - val_f1_macro: 0.4353\n",
      "Epoch 38/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9727\n",
      " - val_f1_macro: 0.4440\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9727 - loss: 0.0653 - val_accuracy: 0.6627 - val_loss: 1.7035 - val_f1_macro: 0.4440\n",
      "Epoch 39/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9781\n",
      " - val_f1_macro: 0.4394\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9781 - loss: 0.0598 - val_accuracy: 0.6701 - val_loss: 1.6467 - val_f1_macro: 0.4394\n",
      "Epoch 40/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9784 - l\n",
      " - val_f1_macro: 0.4410\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9784 - loss: 0.0576 - val_accuracy: 0.6608 - val_loss: 1.7049 - val_f1_macro: 0.4410\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "Best model val_f1_macro for fold 4: 47.13%\n",
      "Training fold 5...\n",
      "Epoch 1/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step   - accuracy: 0.5955 - \n",
      " - val_f1_macro: 0.3909\n",
      "Model saved with val_f1_macro: 0.3909\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 28ms/step - accuracy: 0.5956 - loss: 0.9506 - val_accuracy: 0.5858 - val_loss: 0.9042 - val_f1_macro: 0.3909\n",
      "Epoch 2/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.7992\n",
      " - val_f1_macro: 0.4033\n",
      "Model saved with val_f1_macro: 0.4033\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.7992 - loss: 0.5037 - val_accuracy: 0.6289 - val_loss: 0.8393 - val_f1_macro: 0.4033\n",
      "Epoch 3/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step ep - accuracy: 0.8462 - l\n",
      " - val_f1_macro: 0.4181\n",
      "Model saved with val_f1_macro: 0.4181\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8462 - loss: 0.3847 - val_accuracy: 0.6659 - val_loss: 0.8204 - val_f1_macro: 0.4181\n",
      "Epoch 4/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8672 - l\n",
      " - val_f1_macro: 0.4337\n",
      "Model saved with val_f1_macro: 0.4337\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8672 - loss: 0.3306 - val_accuracy: 0.6784 - val_loss: 0.8215 - val_f1_macro: 0.4337\n",
      "Epoch 5/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.8856 -\n",
      " - val_f1_macro: 0.4146\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.8856 - loss: 0.2884 - val_accuracy: 0.6719 - val_loss: 0.8573 - val_f1_macro: 0.4146\n",
      "Epoch 6/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8944 - l\n",
      " - val_f1_macro: 0.4319\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.8944 - loss: 0.2579 - val_accuracy: 0.6752 - val_loss: 0.8911 - val_f1_macro: 0.4319\n",
      "Epoch 7/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.8993 -\n",
      " - val_f1_macro: 0.4377\n",
      "Model saved with val_f1_macro: 0.4377\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.8993 - loss: 0.2459 - val_accuracy: 0.6761 - val_loss: 0.9364 - val_f1_macro: 0.4377\n",
      "Epoch 8/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step ep - accuracy: 0.9098 - l\n",
      " - val_f1_macro: 0.4177\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9098 - loss: 0.2242 - val_accuracy: 0.6756 - val_loss: 0.9609 - val_f1_macro: 0.4177\n",
      "Epoch 9/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9157 -\n",
      " - val_f1_macro: 0.3974\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9157 - loss: 0.2068 - val_accuracy: 0.6594 - val_loss: 0.9783 - val_f1_macro: 0.3974\n",
      "Epoch 10/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9176 -\n",
      " - val_f1_macro: 0.4126\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9176 - loss: 0.1968 - val_accuracy: 0.6719 - val_loss: 1.0433 - val_f1_macro: 0.4126\n",
      "Epoch 11/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9286 -\n",
      " - val_f1_macro: 0.4344\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9286 - loss: 0.1767 - val_accuracy: 0.6802 - val_loss: 1.0534 - val_f1_macro: 0.4344\n",
      "Epoch 12/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9347 - l\n",
      " - val_f1_macro: 0.3636\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9347 - loss: 0.1621 - val_accuracy: 0.6451 - val_loss: 1.1958 - val_f1_macro: 0.3636\n",
      "Epoch 13/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9367 -\n",
      " - val_f1_macro: 0.4285\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9367 - loss: 0.1605 - val_accuracy: 0.6701 - val_loss: 1.1155 - val_f1_macro: 0.4285\n",
      "Epoch 14/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9393 - l\n",
      " - val_f1_macro: 0.3768\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9393 - loss: 0.1500 - val_accuracy: 0.6566 - val_loss: 1.1699 - val_f1_macro: 0.3768\n",
      "Epoch 15/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9500 -\n",
      " - val_f1_macro: 0.3840\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.9500 - loss: 0.1340 - val_accuracy: 0.6719 - val_loss: 1.2130 - val_f1_macro: 0.3840\n",
      "Epoch 16/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9468\n",
      " - val_f1_macro: 0.3841\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9468 - loss: 0.1330 - val_accuracy: 0.6650 - val_loss: 1.2661 - val_f1_macro: 0.3841\n",
      "Epoch 17/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9545 -\n",
      " - val_f1_macro: 0.4192\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9545 - loss: 0.1195 - val_accuracy: 0.6719 - val_loss: 1.3721 - val_f1_macro: 0.4192\n",
      "Epoch 18/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.95\n",
      " - val_f1_macro: 0.4195\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9533 - loss: 0.1217 - val_accuracy: 0.6724 - val_loss: 1.3706 - val_f1_macro: 0.4195\n",
      "Epoch 19/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9572\n",
      " - val_f1_macro: 0.4216\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9572 - loss: 0.1104 - val_accuracy: 0.6710 - val_loss: 1.3963 - val_f1_macro: 0.4216\n",
      "Epoch 20/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9555 -\n",
      " - val_f1_macro: 0.4191\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9555 - loss: 0.1097 - val_accuracy: 0.6548 - val_loss: 1.3626 - val_f1_macro: 0.4191\n",
      "Epoch 21/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9612 - l\n",
      " - val_f1_macro: 0.4057\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.9612 - loss: 0.0992 - val_accuracy: 0.6696 - val_loss: 1.4121 - val_f1_macro: 0.4057\n",
      "Epoch 22/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9643 -\n",
      " - val_f1_macro: 0.4169\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9643 - loss: 0.0993 - val_accuracy: 0.6668 - val_loss: 1.4551 - val_f1_macro: 0.4169\n",
      "Epoch 23/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9671 -\n",
      " - val_f1_macro: 0.3983\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9671 - loss: 0.0885 - val_accuracy: 0.6622 - val_loss: 1.5146 - val_f1_macro: 0.3983\n",
      "Epoch 24/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9632\n",
      " - val_f1_macro: 0.3950\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9632 - loss: 0.0917 - val_accuracy: 0.6645 - val_loss: 1.5326 - val_f1_macro: 0.3950\n",
      "Epoch 25/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9670 -\n",
      " - val_f1_macro: 0.4023\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9670 - loss: 0.0882 - val_accuracy: 0.6631 - val_loss: 1.6046 - val_f1_macro: 0.4023\n",
      "Epoch 26/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9665 -\n",
      " - val_f1_macro: 0.4021\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9665 - loss: 0.0845 - val_accuracy: 0.6645 - val_loss: 1.5437 - val_f1_macro: 0.4021\n",
      "Epoch 27/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/stepep - accuracy: 0.9680 - lo\n",
      " - val_f1_macro: 0.3931\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9680 - loss: 0.0827 - val_accuracy: 0.6668 - val_loss: 1.6299 - val_f1_macro: 0.3931\n",
      "Epoch 28/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9700\n",
      " - val_f1_macro: 0.3815\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9700 - loss: 0.0821 - val_accuracy: 0.6608 - val_loss: 1.6669 - val_f1_macro: 0.3815\n",
      "Epoch 29/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9682 -\n",
      " - val_f1_macro: 0.4019\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9682 - loss: 0.0813 - val_accuracy: 0.6682 - val_loss: 1.6832 - val_f1_macro: 0.4019\n",
      "Epoch 30/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9727 - l\n",
      " - val_f1_macro: 0.4022\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9727 - loss: 0.0731 - val_accuracy: 0.6677 - val_loss: 1.6295 - val_f1_macro: 0.4022\n",
      "Epoch 31/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9733 -\n",
      " - val_f1_macro: 0.4035\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9733 - loss: 0.0731 - val_accuracy: 0.6682 - val_loss: 1.6739 - val_f1_macro: 0.4035\n",
      "Epoch 32/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9734\n",
      " - val_f1_macro: 0.4049\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9734 - loss: 0.0685 - val_accuracy: 0.6668 - val_loss: 1.7433 - val_f1_macro: 0.4049\n",
      "Epoch 33/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9762 -\n",
      " - val_f1_macro: 0.4062\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9762 - loss: 0.0693 - val_accuracy: 0.6631 - val_loss: 1.7246 - val_f1_macro: 0.4062\n",
      "Epoch 34/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step ep - accuracy: 0.9728 - l\n",
      " - val_f1_macro: 0.3956\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9728 - loss: 0.0686 - val_accuracy: 0.6807 - val_loss: 1.8045 - val_f1_macro: 0.3956\n",
      "Epoch 35/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9732 - l\n",
      " - val_f1_macro: 0.3927\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9732 - loss: 0.0713 - val_accuracy: 0.6576 - val_loss: 1.8503 - val_f1_macro: 0.3927\n",
      "Epoch 36/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9728 -\n",
      " - val_f1_macro: 0.3940\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9728 - loss: 0.0731 - val_accuracy: 0.6576 - val_loss: 1.9470 - val_f1_macro: 0.3940\n",
      "Epoch 37/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step ep - accuracy: 0.\n",
      " - val_f1_macro: 0.3782\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.9757 - loss: 0.0661 - val_accuracy: 0.6701 - val_loss: 1.8274 - val_f1_macro: 0.3782\n",
      "Epoch 38/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step ep - accuracy: 0.9766 -\n",
      " - val_f1_macro: 0.4040\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9766 - loss: 0.0625 - val_accuracy: 0.6608 - val_loss: 1.8880 - val_f1_macro: 0.4040\n",
      "Epoch 39/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step ep - accuracy: 0.97\n",
      " - val_f1_macro: 0.4011\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9747 - loss: 0.0636 - val_accuracy: 0.6724 - val_loss: 1.8248 - val_f1_macro: 0.4011\n",
      "Epoch 40/40\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step ep - accuracy: 0.9770 -\n",
      " - val_f1_macro: 0.3783\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - accuracy: 0.9770 - loss: 0.0598 - val_accuracy: 0.6654 - val_loss: 1.9133 - val_f1_macro: 0.3783\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "Best model val_f1_macro for fold 5: 43.77%\n",
      "Mean F1-macro across all folds: 45.24%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Layer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def prepare_data_with_oversampling(X_images, X_texts, X_images_text, y):\n",
    "    # Over-sampling với các mẫu ít nhất (text-sarcasm)\n",
    "    data = list(zip(X_images, X_texts, X_images_text, y))\n",
    "    data_df = pd.DataFrame(data, columns=['images', 'texts', 'images_text', 'labels'])\n",
    "\n",
    "    # Tách riêng các nhóm theo nhãn\n",
    "    max_label_count = data_df['labels'].value_counts().max()\n",
    "    sampled_data = []\n",
    "    for label in data_df['labels'].unique():\n",
    "        label_data = data_df[data_df['labels'] == label]\n",
    "        # Nếu nhãn ít hơn max_label_count, thực hiện Over-sampling\n",
    "        if len(label_data) < max_label_count:\n",
    "            label_data = resample(label_data, replace=True, n_samples=max_label_count, random_state=42)\n",
    "        sampled_data.append(label_data)\n",
    "    \n",
    "    # Kết hợp lại các mẫu\n",
    "    balanced_data = pd.concat(sampled_data)\n",
    "\n",
    "    # Tách dữ liệu đã over-sampling\n",
    "    X_images_balanced = np.stack(balanced_data['images'].values)\n",
    "    X_texts_balanced = np.stack(balanced_data['texts'].values)\n",
    "    X_images_text_balanced = np.stack(balanced_data['images_text'].values)\n",
    "    y_balanced = np.array(balanced_data['labels'].values)\n",
    "    \n",
    "    return X_images_balanced, X_texts_balanced, X_images_text_balanced, y_balanced\n",
    "\n",
    "def get_class_weights(y):\n",
    "    classes = np.unique(y)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    return class_weights_dict\n",
    "\n",
    "# Custom Self-Attention Layer (unchanged)\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],), initializer=\"zeros\", trainable=True)\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],), initializer=\"glorot_uniform\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.tanh(tf.tensordot(inputs, self.W, axes=[-1, 0]) + self.b)\n",
    "        attention_weights = tf.tensordot(score, self.u, axes=[-1, 0])\n",
    "        attention_weights = tf.nn.softmax(attention_weights, axis=1)\n",
    "        context_vector = attention_weights[..., tf.newaxis] * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SelfAttention, self).get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Custom callback để tính F1-macro sau mỗi epoch và lưu mô hình tốt nhất (unchanged)\n",
    "class F1MacroCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, save_path):\n",
    "        super(F1MacroCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.best_f1 = 0.0  # F1-macro tốt nhất\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        f1 = f1_score(y_val, y_pred_labels, average='macro')\n",
    "        logs['val_f1_macro'] = f1\n",
    "        print(f' - val_f1_macro: {f1:.4f}')\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.model.save(self.save_path)\n",
    "            print(f'Model saved with val_f1_macro: {f1:.4f}')\n",
    "\n",
    "# Tạo mô hình CNN với Self-Attention cho hình ảnh và văn bản (unchanged)\n",
    "def create_model(num_filters=64, filter_size=3, pool_size=1, learning_rate=0.0001):\n",
    "    input_images = Input(shape=(1, 768))\n",
    "    x1 = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu', padding='same')(input_images)\n",
    "    x1 = MaxPooling1D(pool_size=1)(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    x1 = SelfAttention()(x1)\n",
    "    \n",
    "    input_texts = Input(shape=(1, 768))\n",
    "    x2 = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu', padding='same')(input_texts)\n",
    "    x2 = MaxPooling1D(pool_size=1)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x2 = SelfAttention()(x2)\n",
    "    \n",
    "    input_images_text = Input(shape=(1, 768))\n",
    "    x3 = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu', padding='same')(input_images_text)\n",
    "    x3 = MaxPooling1D(pool_size=1)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "    x3 = SelfAttention()(x3)\n",
    "    \n",
    "    combined = concatenate([x1, x2, x3])\n",
    "    output = Dense(4, activation='softmax')(combined)\n",
    "    model = Model(inputs=[input_images, input_texts, input_images_text], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def run_kfold_with_sampling(X_train_images, X_train_texts, X_train_images_text, y_train, n_splits=5):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "    fold_no = 1\n",
    "    all_val_f1 = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(X_train_images):\n",
    "        print(f'Training fold {fold_no}...')\n",
    "\n",
    "        # Tạo dữ liệu huấn luyện với over-sampling\n",
    "        X_train_fold_images, X_train_fold_texts, X_train_fold_images_text, y_train_fold = prepare_data_with_oversampling(\n",
    "            X_train_images[train_idx], \n",
    "            X_train_texts[train_idx], \n",
    "            X_train_images_text[train_idx], \n",
    "            y_train[train_idx]\n",
    "        )\n",
    "\n",
    "        # Chuẩn bị dữ liệu validation\n",
    "        X_val_fold = [X_train_images[val_idx], X_train_texts[val_idx], X_train_images_text[val_idx]]\n",
    "        y_val_fold = y_train[val_idx]\n",
    "\n",
    "        # Tạo mô hình mới cho mỗi fold\n",
    "        model = create_model()\n",
    "\n",
    "        # Đường dẫn lưu mô hình tốt nhất của mỗi fold\n",
    "        checkpoint_filepath = f'best_model_fold_{fold_no}.keras'\n",
    "\n",
    "        # Tính toán class weights\n",
    "        class_weights = get_class_weights(y_train_fold)\n",
    "\n",
    "        # Callback F1-macro để lưu mô hình tốt nhất\n",
    "        f1_macro_callback = F1MacroCallback(validation_data=(X_val_fold, y_val_fold), save_path=checkpoint_filepath)\n",
    "\n",
    "        # Huấn luyện mô hình với class weights và callback\n",
    "        history = model.fit(\n",
    "            [X_train_fold_images, X_train_fold_texts, X_train_fold_images_text], \n",
    "            y_train_fold, \n",
    "            epochs=40, \n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            class_weight=class_weights,  # Weighted Loss\n",
    "            callbacks=[f1_macro_callback]\n",
    "        )\n",
    "\n",
    "        # Đánh giá mô hình tốt nhất trên tập validation\n",
    "        best_model = tf.keras.models.load_model(checkpoint_filepath, custom_objects={'SelfAttention': SelfAttention})\n",
    "\n",
    "        # Tính F1-macro trên tập validation của mô hình tốt nhất\n",
    "        y_pred = best_model.predict(X_val_fold)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        val_f1 = f1_score(y_val_fold, y_pred_labels, average='macro')\n",
    "        print(f'Best model val_f1_macro for fold {fold_no}: {val_f1*100:.2f}%')\n",
    "\n",
    "        all_val_f1.append(val_f1)\n",
    "        fold_no += 1\n",
    "\n",
    "    print(f'Mean F1-macro across all folds: {np.mean(all_val_f1)*100:.2f}%')\n",
    "\n",
    "# Thực hiện huấn luyện K-Fold với over-sampling và weighted loss\n",
    "run_kfold_with_sampling(X_train_images, X_train_texts, X_train_images_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_layer_3', 'input_layer_4', 'input_layer_5']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "[[4.7113471e-02 8.6384475e-01 8.6392276e-02 2.6495077e-03]\n",
      " [2.7035898e-01 6.8785483e-01 3.9151393e-02 2.6347442e-03]\n",
      " [9.4934094e-01 4.2644002e-02 4.8829406e-03 3.1321631e-03]\n",
      " ...\n",
      " [2.5127545e-01 7.1528524e-01 3.0758295e-02 2.6809794e-03]\n",
      " [2.3068228e-01 7.2391027e-01 4.4426341e-02 9.8110281e-04]\n",
      " [6.8013430e-02 8.4772366e-01 8.3442032e-02 8.2085829e-04]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Đọc mô hình đã lưu từ file\n",
    "best_model = tf.keras.models.load_model('E:\\\\best_k_fold_f1_macro\\\\best_model_fold_1_4331.keras', custom_objects={'SelfAttention': SelfAttention})\n",
    "# Đánh giá mô hình trên tập dữ liệu test hoặc validation\n",
    "# Giả sử bạn có dữ liệu test: X_test_images, X_test_texts, X_test_images_text, y_test\n",
    "# test_loss, test_accuracy = best_model.evaluate([X_train_images, X_train_texts, X_train_images_text], y_train)\n",
    "# print(f'Test accuracy: {test_accuracy*100:.2f}%')\n",
    "\n",
    "# Sử dụng mô hình để dự đoán\n",
    "# Giả sử bạn có dữ liệu mới: X_new_images, X_new_texts, X_new_images_text\n",
    "predictions = best_model.predict([X_dev_images, X_dev_texts, X_dev_images_text])\n",
    "\n",
    "# In ra kết quả dự đoán\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:\n",
      " ['multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'image-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'not-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'image-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm', 'multi-sarcasm']\n"
     ]
    }
   ],
   "source": [
    "# Chuyển predictions về dạng nhãn bằng np.argmax\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Mapping các chỉ số thành nhãn\n",
    "label_mapping = {\n",
    "    0: 'not-sarcasm',\n",
    "    1: 'multi-sarcasm',\n",
    "    2: 'image-sarcasm',\n",
    "    3: 'text-sarcasm'\n",
    "}\n",
    "\n",
    "# Chuyển đổi predicted_labels thành nhãn từ label_mapping\n",
    "predicted_label_names = [label_mapping[label] for label in predicted_labels]\n",
    "\n",
    "# In ra nhãn dự đoán\n",
    "print(\"Predicted labels:\\n\", predicted_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image-sarcasm', 'multi-sarcasm', 'not-sarcasm']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(predicted_label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"results\": {\n",
      "        \"0\": \"multi-sarcasm\",\n",
      "        \"1\": \"multi-sarcasm\",\n",
      "        \"2\": \"not-sarcasm\",\n",
      "        \"3\": \"not-sarcasm\",\n",
      "        \"4\": \"not-sarcasm\",\n",
      "        \"5\": \"not-sarcasm\",\n",
      "        \"6\": \"not-sarcasm\",\n",
      "        \"7\": \"not-sarcasm\",\n",
      "        \"8\": \"not-sarcasm\",\n",
      "        \"9\": \"not-sarcasm\",\n",
      "        \"10\": \"multi-sarcasm\",\n",
      "        \"11\": \"multi-sarcasm\",\n",
      "        \"12\": \"not-sarcasm\",\n",
      "        \"13\": \"not-sarcasm\",\n",
      "        \"14\": \"multi-sarcasm\",\n",
      "        \"15\": \"not-sarcasm\",\n",
      "        \"16\": \"not-sarcasm\",\n",
      "        \"17\": \"not-sarcasm\",\n",
      "        \"18\": \"not-sarcasm\",\n",
      "        \"19\": \"not-sarcasm\",\n",
      "        \"20\": \"not-sarcasm\",\n",
      "        \"21\": \"not-sarcasm\",\n",
      "        \"22\": \"not-sarcasm\",\n",
      "        \"23\": \"not-sarcasm\",\n",
      "        \"24\": \"multi-sarcasm\",\n",
      "        \"25\": \"not-sarcasm\",\n",
      "        \"26\": \"not-sarcasm\",\n",
      "        \"27\": \"not-sarcasm\",\n",
      "        \"28\": \"not-sarcasm\",\n",
      "        \"29\": \"not-sarcasm\",\n",
      "        \"30\": \"not-sarcasm\",\n",
      "        \"31\": \"not-sarcasm\",\n",
      "        \"32\": \"multi-sarcasm\",\n",
      "        \"33\": \"multi-sarcasm\",\n",
      "        \"34\": \"not-sarcasm\",\n",
      "        \"35\": \"not-sarcasm\",\n",
      "        \"36\": \"multi-sarcasm\",\n",
      "        \"37\": \"multi-sarcasm\",\n",
      "        \"38\": \"not-sarcasm\",\n",
      "        \"39\": \"not-sarcasm\",\n",
      "        \"40\": \"not-sarcasm\",\n",
      "        \"41\": \"not-sarcasm\",\n",
      "        \"42\": \"not-sarcasm\",\n",
      "        \"43\": \"not-sarcasm\",\n",
      "        \"44\": \"not-sarcasm\",\n",
      "        \"45\": \"not-sarcasm\",\n",
      "        \"46\": \"not-sarcasm\",\n",
      "        \"47\": \"multi-sarcasm\",\n",
      "        \"48\": \"multi-sarcasm\",\n",
      "        \"49\": \"multi-sarcasm\",\n",
      "        \"50\": \"not-sarcasm\",\n",
      "        \"51\": \"multi-sarcasm\",\n",
      "        \"52\": \"multi-sarcasm\",\n",
      "        \"53\": \"not-sarcasm\",\n",
      "        \"54\": \"multi-sarcasm\",\n",
      "        \"55\": \"not-sarcasm\",\n",
      "        \"56\": \"not-sarcasm\",\n",
      "        \"57\": \"not-sarcasm\",\n",
      "        \"58\": \"not-sarcasm\",\n",
      "        \"59\": \"not-sarcasm\",\n",
      "        \"60\": \"not-sarcasm\",\n",
      "        \"61\": \"not-sarcasm\",\n",
      "        \"62\": \"multi-sarcasm\",\n",
      "        \"63\": \"not-sarcasm\",\n",
      "        \"64\": \"multi-sarcasm\",\n",
      "        \"65\": \"multi-sarcasm\",\n",
      "        \"66\": \"multi-sarcasm\",\n",
      "        \"67\": \"multi-sarcasm\",\n",
      "        \"68\": \"multi-sarcasm\",\n",
      "        \"69\": \"multi-sarcasm\",\n",
      "        \"70\": \"multi-sarcasm\",\n",
      "        \"71\": \"multi-sarcasm\",\n",
      "        \"72\": \"multi-sarcasm\",\n",
      "        \"73\": \"not-sarcasm\",\n",
      "        \"74\": \"not-sarcasm\",\n",
      "        \"75\": \"multi-sarcasm\",\n",
      "        \"76\": \"multi-sarcasm\",\n",
      "        \"77\": \"multi-sarcasm\",\n",
      "        \"78\": \"multi-sarcasm\",\n",
      "        \"79\": \"not-sarcasm\",\n",
      "        \"80\": \"not-sarcasm\",\n",
      "        \"81\": \"multi-sarcasm\",\n",
      "        \"82\": \"not-sarcasm\",\n",
      "        \"83\": \"multi-sarcasm\",\n",
      "        \"84\": \"not-sarcasm\",\n",
      "        \"85\": \"multi-sarcasm\",\n",
      "        \"86\": \"not-sarcasm\",\n",
      "        \"87\": \"not-sarcasm\",\n",
      "        \"88\": \"not-sarcasm\",\n",
      "        \"89\": \"multi-sarcasm\",\n",
      "        \"90\": \"multi-sarcasm\",\n",
      "        \"91\": \"multi-sarcasm\",\n",
      "        \"92\": \"multi-sarcasm\",\n",
      "        \"93\": \"multi-sarcasm\",\n",
      "        \"94\": \"multi-sarcasm\",\n",
      "        \"95\": \"multi-sarcasm\",\n",
      "        \"96\": \"not-sarcasm\",\n",
      "        \"97\": \"image-sarcasm\",\n",
      "        \"98\": \"multi-sarcasm\",\n",
      "        \"99\": \"not-sarcasm\",\n",
      "        \"100\": \"not-sarcasm\",\n",
      "        \"101\": \"not-sarcasm\",\n",
      "        \"102\": \"not-sarcasm\",\n",
      "        \"103\": \"multi-sarcasm\",\n",
      "        \"104\": \"multi-sarcasm\",\n",
      "        \"105\": \"not-sarcasm\",\n",
      "        \"106\": \"not-sarcasm\",\n",
      "        \"107\": \"not-sarcasm\",\n",
      "        \"108\": \"not-sarcasm\",\n",
      "        \"109\": \"multi-sarcasm\",\n",
      "        \"110\": \"not-sarcasm\",\n",
      "        \"111\": \"not-sarcasm\",\n",
      "        \"112\": \"multi-sarcasm\",\n",
      "        \"113\": \"not-sarcasm\",\n",
      "        \"114\": \"multi-sarcasm\",\n",
      "        \"115\": \"multi-sarcasm\",\n",
      "        \"116\": \"not-sarcasm\",\n",
      "        \"117\": \"multi-sarcasm\",\n",
      "        \"118\": \"multi-sarcasm\",\n",
      "        \"119\": \"multi-sarcasm\",\n",
      "        \"120\": \"not-sarcasm\",\n",
      "        \"121\": \"not-sarcasm\",\n",
      "        \"122\": \"not-sarcasm\",\n",
      "        \"123\": \"not-sarcasm\",\n",
      "        \"124\": \"not-sarcasm\",\n",
      "        \"125\": \"not-sarcasm\",\n",
      "        \"126\": \"not-sarcasm\",\n",
      "        \"127\": \"not-sarcasm\",\n",
      "        \"128\": \"not-sarcasm\",\n",
      "        \"129\": \"not-sarcasm\",\n",
      "        \"130\": \"multi-sarcasm\",\n",
      "        \"131\": \"not-sarcasm\",\n",
      "        \"132\": \"not-sarcasm\",\n",
      "        \"133\": \"not-sarcasm\",\n",
      "        \"134\": \"not-sarcasm\",\n",
      "        \"135\": \"not-sarcasm\",\n",
      "        \"136\": \"not-sarcasm\",\n",
      "        \"137\": \"not-sarcasm\",\n",
      "        \"138\": \"not-sarcasm\",\n",
      "        \"139\": \"multi-sarcasm\",\n",
      "        \"140\": \"not-sarcasm\",\n",
      "        \"141\": \"not-sarcasm\",\n",
      "        \"142\": \"not-sarcasm\",\n",
      "        \"143\": \"not-sarcasm\",\n",
      "        \"144\": \"not-sarcasm\",\n",
      "        \"145\": \"multi-sarcasm\",\n",
      "        \"146\": \"not-sarcasm\",\n",
      "        \"147\": \"not-sarcasm\",\n",
      "        \"148\": \"not-sarcasm\",\n",
      "        \"149\": \"not-sarcasm\",\n",
      "        \"150\": \"not-sarcasm\",\n",
      "        \"151\": \"multi-sarcasm\",\n",
      "        \"152\": \"multi-sarcasm\",\n",
      "        \"153\": \"multi-sarcasm\",\n",
      "        \"154\": \"not-sarcasm\",\n",
      "        \"155\": \"multi-sarcasm\",\n",
      "        \"156\": \"multi-sarcasm\",\n",
      "        \"157\": \"not-sarcasm\",\n",
      "        \"158\": \"multi-sarcasm\",\n",
      "        \"159\": \"multi-sarcasm\",\n",
      "        \"160\": \"not-sarcasm\",\n",
      "        \"161\": \"not-sarcasm\",\n",
      "        \"162\": \"multi-sarcasm\",\n",
      "        \"163\": \"multi-sarcasm\",\n",
      "        \"164\": \"not-sarcasm\",\n",
      "        \"165\": \"multi-sarcasm\",\n",
      "        \"166\": \"not-sarcasm\",\n",
      "        \"167\": \"multi-sarcasm\",\n",
      "        \"168\": \"multi-sarcasm\",\n",
      "        \"169\": \"multi-sarcasm\",\n",
      "        \"170\": \"multi-sarcasm\",\n",
      "        \"171\": \"multi-sarcasm\",\n",
      "        \"172\": \"multi-sarcasm\",\n",
      "        \"173\": \"multi-sarcasm\",\n",
      "        \"174\": \"not-sarcasm\",\n",
      "        \"175\": \"not-sarcasm\",\n",
      "        \"176\": \"not-sarcasm\",\n",
      "        \"177\": \"not-sarcasm\",\n",
      "        \"178\": \"not-sarcasm\",\n",
      "        \"179\": \"multi-sarcasm\",\n",
      "        \"180\": \"multi-sarcasm\",\n",
      "        \"181\": \"not-sarcasm\",\n",
      "        \"182\": \"not-sarcasm\",\n",
      "        \"183\": \"not-sarcasm\",\n",
      "        \"184\": \"not-sarcasm\",\n",
      "        \"185\": \"multi-sarcasm\",\n",
      "        \"186\": \"multi-sarcasm\",\n",
      "        \"187\": \"not-sarcasm\",\n",
      "        \"188\": \"not-sarcasm\",\n",
      "        \"189\": \"not-sarcasm\",\n",
      "        \"190\": \"not-sarcasm\",\n",
      "        \"191\": \"multi-sarcasm\",\n",
      "        \"192\": \"multi-sarcasm\",\n",
      "        \"193\": \"multi-sarcasm\",\n",
      "        \"194\": \"multi-sarcasm\",\n",
      "        \"195\": \"multi-sarcasm\",\n",
      "        \"196\": \"not-sarcasm\",\n",
      "        \"197\": \"multi-sarcasm\",\n",
      "        \"198\": \"not-sarcasm\",\n",
      "        \"199\": \"not-sarcasm\",\n",
      "        \"200\": \"not-sarcasm\",\n",
      "        \"201\": \"not-sarcasm\",\n",
      "        \"202\": \"not-sarcasm\",\n",
      "        \"203\": \"not-sarcasm\",\n",
      "        \"204\": \"multi-sarcasm\",\n",
      "        \"205\": \"not-sarcasm\",\n",
      "        \"206\": \"multi-sarcasm\",\n",
      "        \"207\": \"not-sarcasm\",\n",
      "        \"208\": \"not-sarcasm\",\n",
      "        \"209\": \"not-sarcasm\",\n",
      "        \"210\": \"not-sarcasm\",\n",
      "        \"211\": \"not-sarcasm\",\n",
      "        \"212\": \"multi-sarcasm\",\n",
      "        \"213\": \"not-sarcasm\",\n",
      "        \"214\": \"not-sarcasm\",\n",
      "        \"215\": \"multi-sarcasm\",\n",
      "        \"216\": \"not-sarcasm\",\n",
      "        \"217\": \"not-sarcasm\",\n",
      "        \"218\": \"not-sarcasm\",\n",
      "        \"219\": \"not-sarcasm\",\n",
      "        \"220\": \"not-sarcasm\",\n",
      "        \"221\": \"not-sarcasm\",\n",
      "        \"222\": \"not-sarcasm\",\n",
      "        \"223\": \"multi-sarcasm\",\n",
      "        \"224\": \"multi-sarcasm\",\n",
      "        \"225\": \"not-sarcasm\",\n",
      "        \"226\": \"not-sarcasm\",\n",
      "        \"227\": \"multi-sarcasm\",\n",
      "        \"228\": \"multi-sarcasm\",\n",
      "        \"229\": \"multi-sarcasm\",\n",
      "        \"230\": \"not-sarcasm\",\n",
      "        \"231\": \"multi-sarcasm\",\n",
      "        \"232\": \"multi-sarcasm\",\n",
      "        \"233\": \"multi-sarcasm\",\n",
      "        \"234\": \"not-sarcasm\",\n",
      "        \"235\": \"not-sarcasm\",\n",
      "        \"236\": \"not-sarcasm\",\n",
      "        \"237\": \"not-sarcasm\",\n",
      "        \"238\": \"not-sarcasm\",\n",
      "        \"239\": \"not-sarcasm\",\n",
      "        \"240\": \"not-sarcasm\",\n",
      "        \"241\": \"not-sarcasm\",\n",
      "        \"242\": \"not-sarcasm\",\n",
      "        \"243\": \"not-sarcasm\",\n",
      "        \"244\": \"not-sarcasm\",\n",
      "        \"245\": \"not-sarcasm\",\n",
      "        \"246\": \"multi-sarcasm\",\n",
      "        \"247\": \"multi-sarcasm\",\n",
      "        \"248\": \"multi-sarcasm\",\n",
      "        \"249\": \"not-sarcasm\",\n",
      "        \"250\": \"not-sarcasm\",\n",
      "        \"251\": \"not-sarcasm\",\n",
      "        \"252\": \"not-sarcasm\",\n",
      "        \"253\": \"multi-sarcasm\",\n",
      "        \"254\": \"not-sarcasm\",\n",
      "        \"255\": \"not-sarcasm\",\n",
      "        \"256\": \"not-sarcasm\",\n",
      "        \"257\": \"not-sarcasm\",\n",
      "        \"258\": \"not-sarcasm\",\n",
      "        \"259\": \"not-sarcasm\",\n",
      "        \"260\": \"not-sarcasm\",\n",
      "        \"261\": \"multi-sarcasm\",\n",
      "        \"262\": \"not-sarcasm\",\n",
      "        \"263\": \"multi-sarcasm\",\n",
      "        \"264\": \"multi-sarcasm\",\n",
      "        \"265\": \"not-sarcasm\",\n",
      "        \"266\": \"multi-sarcasm\",\n",
      "        \"267\": \"not-sarcasm\",\n",
      "        \"268\": \"multi-sarcasm\",\n",
      "        \"269\": \"multi-sarcasm\",\n",
      "        \"270\": \"multi-sarcasm\",\n",
      "        \"271\": \"multi-sarcasm\",\n",
      "        \"272\": \"multi-sarcasm\",\n",
      "        \"273\": \"multi-sarcasm\",\n",
      "        \"274\": \"multi-sarcasm\",\n",
      "        \"275\": \"multi-sarcasm\",\n",
      "        \"276\": \"not-sarcasm\",\n",
      "        \"277\": \"multi-sarcasm\",\n",
      "        \"278\": \"multi-sarcasm\",\n",
      "        \"279\": \"multi-sarcasm\",\n",
      "        \"280\": \"not-sarcasm\",\n",
      "        \"281\": \"not-sarcasm\",\n",
      "        \"282\": \"multi-sarcasm\",\n",
      "        \"283\": \"not-sarcasm\",\n",
      "        \"284\": \"not-sarcasm\",\n",
      "        \"285\": \"not-sarcasm\",\n",
      "        \"286\": \"multi-sarcasm\",\n",
      "        \"287\": \"multi-sarcasm\",\n",
      "        \"288\": \"not-sarcasm\",\n",
      "        \"289\": \"not-sarcasm\",\n",
      "        \"290\": \"not-sarcasm\",\n",
      "        \"291\": \"multi-sarcasm\",\n",
      "        \"292\": \"multi-sarcasm\",\n",
      "        \"293\": \"not-sarcasm\",\n",
      "        \"294\": \"not-sarcasm\",\n",
      "        \"295\": \"multi-sarcasm\",\n",
      "        \"296\": \"multi-sarcasm\",\n",
      "        \"297\": \"multi-sarcasm\",\n",
      "        \"298\": \"multi-sarcasm\",\n",
      "        \"299\": \"multi-sarcasm\",\n",
      "        \"300\": \"not-sarcasm\",\n",
      "        \"301\": \"not-sarcasm\",\n",
      "        \"302\": \"not-sarcasm\",\n",
      "        \"303\": \"not-sarcasm\",\n",
      "        \"304\": \"not-sarcasm\",\n",
      "        \"305\": \"not-sarcasm\",\n",
      "        \"306\": \"not-sarcasm\",\n",
      "        \"307\": \"multi-sarcasm\",\n",
      "        \"308\": \"multi-sarcasm\",\n",
      "        \"309\": \"not-sarcasm\",\n",
      "        \"310\": \"multi-sarcasm\",\n",
      "        \"311\": \"multi-sarcasm\",\n",
      "        \"312\": \"multi-sarcasm\",\n",
      "        \"313\": \"not-sarcasm\",\n",
      "        \"314\": \"multi-sarcasm\",\n",
      "        \"315\": \"multi-sarcasm\",\n",
      "        \"316\": \"not-sarcasm\",\n",
      "        \"317\": \"not-sarcasm\",\n",
      "        \"318\": \"not-sarcasm\",\n",
      "        \"319\": \"not-sarcasm\",\n",
      "        \"320\": \"multi-sarcasm\",\n",
      "        \"321\": \"multi-sarcasm\",\n",
      "        \"322\": \"multi-sarcasm\",\n",
      "        \"323\": \"multi-sarcasm\",\n",
      "        \"324\": \"multi-sarcasm\",\n",
      "        \"325\": \"not-sarcasm\",\n",
      "        \"326\": \"not-sarcasm\",\n",
      "        \"327\": \"multi-sarcasm\",\n",
      "        \"328\": \"not-sarcasm\",\n",
      "        \"329\": \"image-sarcasm\",\n",
      "        \"330\": \"not-sarcasm\",\n",
      "        \"331\": \"not-sarcasm\",\n",
      "        \"332\": \"not-sarcasm\",\n",
      "        \"333\": \"not-sarcasm\",\n",
      "        \"334\": \"not-sarcasm\",\n",
      "        \"335\": \"not-sarcasm\",\n",
      "        \"336\": \"not-sarcasm\",\n",
      "        \"337\": \"not-sarcasm\",\n",
      "        \"338\": \"multi-sarcasm\",\n",
      "        \"339\": \"multi-sarcasm\",\n",
      "        \"340\": \"not-sarcasm\",\n",
      "        \"341\": \"not-sarcasm\",\n",
      "        \"342\": \"multi-sarcasm\",\n",
      "        \"343\": \"multi-sarcasm\",\n",
      "        \"344\": \"not-sarcasm\",\n",
      "        \"345\": \"not-sarcasm\",\n",
      "        \"346\": \"not-sarcasm\",\n",
      "        \"347\": \"not-sarcasm\",\n",
      "        \"348\": \"not-sarcasm\",\n",
      "        \"349\": \"not-sarcasm\",\n",
      "        \"350\": \"not-sarcasm\",\n",
      "        \"351\": \"not-sarcasm\",\n",
      "        \"352\": \"multi-sarcasm\",\n",
      "        \"353\": \"not-sarcasm\",\n",
      "        \"354\": \"not-sarcasm\",\n",
      "        \"355\": \"not-sarcasm\",\n",
      "        \"356\": \"multi-sarcasm\",\n",
      "        \"357\": \"not-sarcasm\",\n",
      "        \"358\": \"not-sarcasm\",\n",
      "        \"359\": \"not-sarcasm\",\n",
      "        \"360\": \"not-sarcasm\",\n",
      "        \"361\": \"not-sarcasm\",\n",
      "        \"362\": \"not-sarcasm\",\n",
      "        \"363\": \"multi-sarcasm\",\n",
      "        \"364\": \"multi-sarcasm\",\n",
      "        \"365\": \"not-sarcasm\",\n",
      "        \"366\": \"not-sarcasm\",\n",
      "        \"367\": \"not-sarcasm\",\n",
      "        \"368\": \"not-sarcasm\",\n",
      "        \"369\": \"not-sarcasm\",\n",
      "        \"370\": \"not-sarcasm\",\n",
      "        \"371\": \"not-sarcasm\",\n",
      "        \"372\": \"not-sarcasm\",\n",
      "        \"373\": \"not-sarcasm\",\n",
      "        \"374\": \"not-sarcasm\",\n",
      "        \"375\": \"multi-sarcasm\",\n",
      "        \"376\": \"multi-sarcasm\",\n",
      "        \"377\": \"not-sarcasm\",\n",
      "        \"378\": \"not-sarcasm\",\n",
      "        \"379\": \"multi-sarcasm\",\n",
      "        \"380\": \"not-sarcasm\",\n",
      "        \"381\": \"multi-sarcasm\",\n",
      "        \"382\": \"not-sarcasm\",\n",
      "        \"383\": \"not-sarcasm\",\n",
      "        \"384\": \"not-sarcasm\",\n",
      "        \"385\": \"not-sarcasm\",\n",
      "        \"386\": \"not-sarcasm\",\n",
      "        \"387\": \"not-sarcasm\",\n",
      "        \"388\": \"not-sarcasm\",\n",
      "        \"389\": \"image-sarcasm\",\n",
      "        \"390\": \"multi-sarcasm\",\n",
      "        \"391\": \"not-sarcasm\",\n",
      "        \"392\": \"multi-sarcasm\",\n",
      "        \"393\": \"multi-sarcasm\",\n",
      "        \"394\": \"not-sarcasm\",\n",
      "        \"395\": \"multi-sarcasm\",\n",
      "        \"396\": \"not-sarcasm\",\n",
      "        \"397\": \"not-sarcasm\",\n",
      "        \"398\": \"not-sarcasm\",\n",
      "        \"399\": \"not-sarcasm\",\n",
      "        \"400\": \"multi-sarcasm\",\n",
      "        \"401\": \"not-sarcasm\",\n",
      "        \"402\": \"not-sarcasm\",\n",
      "        \"403\": \"not-sarcasm\",\n",
      "        \"404\": \"multi-sarcasm\",\n",
      "        \"405\": \"not-sarcasm\",\n",
      "        \"406\": \"not-sarcasm\",\n",
      "        \"407\": \"multi-sarcasm\",\n",
      "        \"408\": \"not-sarcasm\",\n",
      "        \"409\": \"not-sarcasm\",\n",
      "        \"410\": \"not-sarcasm\",\n",
      "        \"411\": \"image-sarcasm\",\n",
      "        \"412\": \"multi-sarcasm\",\n",
      "        \"413\": \"not-sarcasm\",\n",
      "        \"414\": \"multi-sarcasm\",\n",
      "        \"415\": \"multi-sarcasm\",\n",
      "        \"416\": \"multi-sarcasm\",\n",
      "        \"417\": \"not-sarcasm\",\n",
      "        \"418\": \"multi-sarcasm\",\n",
      "        \"419\": \"multi-sarcasm\",\n",
      "        \"420\": \"multi-sarcasm\",\n",
      "        \"421\": \"not-sarcasm\",\n",
      "        \"422\": \"not-sarcasm\",\n",
      "        \"423\": \"not-sarcasm\",\n",
      "        \"424\": \"not-sarcasm\",\n",
      "        \"425\": \"not-sarcasm\",\n",
      "        \"426\": \"not-sarcasm\",\n",
      "        \"427\": \"not-sarcasm\",\n",
      "        \"428\": \"multi-sarcasm\",\n",
      "        \"429\": \"not-sarcasm\",\n",
      "        \"430\": \"not-sarcasm\",\n",
      "        \"431\": \"not-sarcasm\",\n",
      "        \"432\": \"multi-sarcasm\",\n",
      "        \"433\": \"multi-sarcasm\",\n",
      "        \"434\": \"not-sarcasm\",\n",
      "        \"435\": \"not-sarcasm\",\n",
      "        \"436\": \"multi-sarcasm\",\n",
      "        \"437\": \"multi-sarcasm\",\n",
      "        \"438\": \"not-sarcasm\",\n",
      "        \"439\": \"multi-sarcasm\",\n",
      "        \"440\": \"multi-sarcasm\",\n",
      "        \"441\": \"multi-sarcasm\",\n",
      "        \"442\": \"multi-sarcasm\",\n",
      "        \"443\": \"multi-sarcasm\",\n",
      "        \"444\": \"not-sarcasm\",\n",
      "        \"445\": \"not-sarcasm\",\n",
      "        \"446\": \"not-sarcasm\",\n",
      "        \"447\": \"multi-sarcasm\",\n",
      "        \"448\": \"multi-sarcasm\",\n",
      "        \"449\": \"not-sarcasm\",\n",
      "        \"450\": \"multi-sarcasm\",\n",
      "        \"451\": \"not-sarcasm\",\n",
      "        \"452\": \"not-sarcasm\",\n",
      "        \"453\": \"not-sarcasm\",\n",
      "        \"454\": \"not-sarcasm\",\n",
      "        \"455\": \"not-sarcasm\",\n",
      "        \"456\": \"multi-sarcasm\",\n",
      "        \"457\": \"not-sarcasm\",\n",
      "        \"458\": \"multi-sarcasm\",\n",
      "        \"459\": \"not-sarcasm\",\n",
      "        \"460\": \"not-sarcasm\",\n",
      "        \"461\": \"not-sarcasm\",\n",
      "        \"462\": \"not-sarcasm\",\n",
      "        \"463\": \"not-sarcasm\",\n",
      "        \"464\": \"not-sarcasm\",\n",
      "        \"465\": \"multi-sarcasm\",\n",
      "        \"466\": \"multi-sarcasm\",\n",
      "        \"467\": \"multi-sarcasm\",\n",
      "        \"468\": \"not-sarcasm\",\n",
      "        \"469\": \"multi-sarcasm\",\n",
      "        \"470\": \"multi-sarcasm\",\n",
      "        \"471\": \"multi-sarcasm\",\n",
      "        \"472\": \"not-sarcasm\",\n",
      "        \"473\": \"multi-sarcasm\",\n",
      "        \"474\": \"multi-sarcasm\",\n",
      "        \"475\": \"not-sarcasm\",\n",
      "        \"476\": \"not-sarcasm\",\n",
      "        \"477\": \"not-sarcasm\",\n",
      "        \"478\": \"not-sarcasm\",\n",
      "        \"479\": \"not-sarcasm\",\n",
      "        \"480\": \"not-sarcasm\",\n",
      "        \"481\": \"not-sarcasm\",\n",
      "        \"482\": \"multi-sarcasm\",\n",
      "        \"483\": \"multi-sarcasm\",\n",
      "        \"484\": \"multi-sarcasm\",\n",
      "        \"485\": \"multi-sarcasm\",\n",
      "        \"486\": \"multi-sarcasm\",\n",
      "        \"487\": \"not-sarcasm\",\n",
      "        \"488\": \"multi-sarcasm\",\n",
      "        \"489\": \"not-sarcasm\",\n",
      "        \"490\": \"multi-sarcasm\",\n",
      "        \"491\": \"not-sarcasm\",\n",
      "        \"492\": \"multi-sarcasm\",\n",
      "        \"493\": \"multi-sarcasm\",\n",
      "        \"494\": \"multi-sarcasm\",\n",
      "        \"495\": \"multi-sarcasm\",\n",
      "        \"496\": \"multi-sarcasm\",\n",
      "        \"497\": \"multi-sarcasm\",\n",
      "        \"498\": \"not-sarcasm\",\n",
      "        \"499\": \"not-sarcasm\",\n",
      "        \"500\": \"not-sarcasm\",\n",
      "        \"501\": \"multi-sarcasm\",\n",
      "        \"502\": \"multi-sarcasm\",\n",
      "        \"503\": \"multi-sarcasm\",\n",
      "        \"504\": \"multi-sarcasm\",\n",
      "        \"505\": \"multi-sarcasm\",\n",
      "        \"506\": \"multi-sarcasm\",\n",
      "        \"507\": \"multi-sarcasm\",\n",
      "        \"508\": \"multi-sarcasm\",\n",
      "        \"509\": \"not-sarcasm\",\n",
      "        \"510\": \"not-sarcasm\",\n",
      "        \"511\": \"not-sarcasm\",\n",
      "        \"512\": \"multi-sarcasm\",\n",
      "        \"513\": \"multi-sarcasm\",\n",
      "        \"514\": \"multi-sarcasm\",\n",
      "        \"515\": \"multi-sarcasm\",\n",
      "        \"516\": \"multi-sarcasm\",\n",
      "        \"517\": \"not-sarcasm\",\n",
      "        \"518\": \"multi-sarcasm\",\n",
      "        \"519\": \"not-sarcasm\",\n",
      "        \"520\": \"multi-sarcasm\",\n",
      "        \"521\": \"multi-sarcasm\",\n",
      "        \"522\": \"multi-sarcasm\",\n",
      "        \"523\": \"multi-sarcasm\",\n",
      "        \"524\": \"multi-sarcasm\",\n",
      "        \"525\": \"multi-sarcasm\",\n",
      "        \"526\": \"multi-sarcasm\",\n",
      "        \"527\": \"not-sarcasm\",\n",
      "        \"528\": \"multi-sarcasm\",\n",
      "        \"529\": \"multi-sarcasm\",\n",
      "        \"530\": \"multi-sarcasm\",\n",
      "        \"531\": \"multi-sarcasm\",\n",
      "        \"532\": \"multi-sarcasm\",\n",
      "        \"533\": \"multi-sarcasm\",\n",
      "        \"534\": \"not-sarcasm\",\n",
      "        \"535\": \"not-sarcasm\",\n",
      "        \"536\": \"multi-sarcasm\",\n",
      "        \"537\": \"multi-sarcasm\",\n",
      "        \"538\": \"not-sarcasm\",\n",
      "        \"539\": \"not-sarcasm\",\n",
      "        \"540\": \"not-sarcasm\",\n",
      "        \"541\": \"not-sarcasm\",\n",
      "        \"542\": \"not-sarcasm\",\n",
      "        \"543\": \"not-sarcasm\",\n",
      "        \"544\": \"multi-sarcasm\",\n",
      "        \"545\": \"multi-sarcasm\",\n",
      "        \"546\": \"multi-sarcasm\",\n",
      "        \"547\": \"multi-sarcasm\",\n",
      "        \"548\": \"multi-sarcasm\",\n",
      "        \"549\": \"multi-sarcasm\",\n",
      "        \"550\": \"multi-sarcasm\",\n",
      "        \"551\": \"multi-sarcasm\",\n",
      "        \"552\": \"multi-sarcasm\",\n",
      "        \"553\": \"multi-sarcasm\",\n",
      "        \"554\": \"multi-sarcasm\",\n",
      "        \"555\": \"multi-sarcasm\",\n",
      "        \"556\": \"multi-sarcasm\",\n",
      "        \"557\": \"multi-sarcasm\",\n",
      "        \"558\": \"image-sarcasm\",\n",
      "        \"559\": \"multi-sarcasm\",\n",
      "        \"560\": \"multi-sarcasm\",\n",
      "        \"561\": \"multi-sarcasm\",\n",
      "        \"562\": \"multi-sarcasm\",\n",
      "        \"563\": \"multi-sarcasm\",\n",
      "        \"564\": \"multi-sarcasm\",\n",
      "        \"565\": \"multi-sarcasm\",\n",
      "        \"566\": \"not-sarcasm\",\n",
      "        \"567\": \"multi-sarcasm\",\n",
      "        \"568\": \"multi-sarcasm\",\n",
      "        \"569\": \"multi-sarcasm\",\n",
      "        \"570\": \"multi-sarcasm\",\n",
      "        \"571\": \"multi-sarcasm\",\n",
      "        \"572\": \"multi-sarcasm\",\n",
      "        \"573\": \"multi-sarcasm\",\n",
      "        \"574\": \"multi-sarcasm\",\n",
      "        \"575\": \"multi-sarcasm\",\n",
      "        \"576\": \"multi-sarcasm\",\n",
      "        \"577\": \"multi-sarcasm\",\n",
      "        \"578\": \"multi-sarcasm\",\n",
      "        \"579\": \"multi-sarcasm\",\n",
      "        \"580\": \"multi-sarcasm\",\n",
      "        \"581\": \"multi-sarcasm\",\n",
      "        \"582\": \"multi-sarcasm\",\n",
      "        \"583\": \"multi-sarcasm\",\n",
      "        \"584\": \"multi-sarcasm\",\n",
      "        \"585\": \"multi-sarcasm\",\n",
      "        \"586\": \"multi-sarcasm\",\n",
      "        \"587\": \"multi-sarcasm\",\n",
      "        \"588\": \"multi-sarcasm\",\n",
      "        \"589\": \"multi-sarcasm\",\n",
      "        \"590\": \"multi-sarcasm\",\n",
      "        \"591\": \"multi-sarcasm\",\n",
      "        \"592\": \"multi-sarcasm\",\n",
      "        \"593\": \"not-sarcasm\",\n",
      "        \"594\": \"multi-sarcasm\",\n",
      "        \"595\": \"multi-sarcasm\",\n",
      "        \"596\": \"multi-sarcasm\",\n",
      "        \"597\": \"multi-sarcasm\",\n",
      "        \"598\": \"multi-sarcasm\",\n",
      "        \"599\": \"multi-sarcasm\",\n",
      "        \"600\": \"multi-sarcasm\",\n",
      "        \"601\": \"multi-sarcasm\",\n",
      "        \"602\": \"multi-sarcasm\",\n",
      "        \"603\": \"multi-sarcasm\",\n",
      "        \"604\": \"multi-sarcasm\",\n",
      "        \"605\": \"multi-sarcasm\",\n",
      "        \"606\": \"multi-sarcasm\",\n",
      "        \"607\": \"multi-sarcasm\",\n",
      "        \"608\": \"multi-sarcasm\",\n",
      "        \"609\": \"multi-sarcasm\",\n",
      "        \"610\": \"not-sarcasm\",\n",
      "        \"611\": \"not-sarcasm\",\n",
      "        \"612\": \"multi-sarcasm\",\n",
      "        \"613\": \"not-sarcasm\",\n",
      "        \"614\": \"multi-sarcasm\",\n",
      "        \"615\": \"not-sarcasm\",\n",
      "        \"616\": \"not-sarcasm\",\n",
      "        \"617\": \"not-sarcasm\",\n",
      "        \"618\": \"multi-sarcasm\",\n",
      "        \"619\": \"multi-sarcasm\",\n",
      "        \"620\": \"multi-sarcasm\",\n",
      "        \"621\": \"multi-sarcasm\",\n",
      "        \"622\": \"multi-sarcasm\",\n",
      "        \"623\": \"multi-sarcasm\",\n",
      "        \"624\": \"multi-sarcasm\",\n",
      "        \"625\": \"not-sarcasm\",\n",
      "        \"626\": \"multi-sarcasm\",\n",
      "        \"627\": \"image-sarcasm\",\n",
      "        \"628\": \"multi-sarcasm\",\n",
      "        \"629\": \"not-sarcasm\",\n",
      "        \"630\": \"multi-sarcasm\",\n",
      "        \"631\": \"multi-sarcasm\",\n",
      "        \"632\": \"multi-sarcasm\",\n",
      "        \"633\": \"not-sarcasm\",\n",
      "        \"634\": \"multi-sarcasm\",\n",
      "        \"635\": \"multi-sarcasm\",\n",
      "        \"636\": \"multi-sarcasm\",\n",
      "        \"637\": \"multi-sarcasm\",\n",
      "        \"638\": \"multi-sarcasm\",\n",
      "        \"639\": \"not-sarcasm\",\n",
      "        \"640\": \"multi-sarcasm\",\n",
      "        \"641\": \"not-sarcasm\",\n",
      "        \"642\": \"multi-sarcasm\",\n",
      "        \"643\": \"multi-sarcasm\",\n",
      "        \"644\": \"multi-sarcasm\",\n",
      "        \"645\": \"multi-sarcasm\",\n",
      "        \"646\": \"multi-sarcasm\",\n",
      "        \"647\": \"multi-sarcasm\",\n",
      "        \"648\": \"multi-sarcasm\",\n",
      "        \"649\": \"multi-sarcasm\",\n",
      "        \"650\": \"multi-sarcasm\",\n",
      "        \"651\": \"multi-sarcasm\",\n",
      "        \"652\": \"multi-sarcasm\",\n",
      "        \"653\": \"multi-sarcasm\",\n",
      "        \"654\": \"multi-sarcasm\",\n",
      "        \"655\": \"not-sarcasm\",\n",
      "        \"656\": \"multi-sarcasm\",\n",
      "        \"657\": \"multi-sarcasm\",\n",
      "        \"658\": \"not-sarcasm\",\n",
      "        \"659\": \"multi-sarcasm\",\n",
      "        \"660\": \"multi-sarcasm\",\n",
      "        \"661\": \"multi-sarcasm\",\n",
      "        \"662\": \"multi-sarcasm\",\n",
      "        \"663\": \"multi-sarcasm\",\n",
      "        \"664\": \"multi-sarcasm\",\n",
      "        \"665\": \"not-sarcasm\",\n",
      "        \"666\": \"multi-sarcasm\",\n",
      "        \"667\": \"multi-sarcasm\",\n",
      "        \"668\": \"multi-sarcasm\",\n",
      "        \"669\": \"not-sarcasm\",\n",
      "        \"670\": \"multi-sarcasm\",\n",
      "        \"671\": \"not-sarcasm\",\n",
      "        \"672\": \"not-sarcasm\",\n",
      "        \"673\": \"multi-sarcasm\",\n",
      "        \"674\": \"not-sarcasm\",\n",
      "        \"675\": \"not-sarcasm\",\n",
      "        \"676\": \"multi-sarcasm\",\n",
      "        \"677\": \"not-sarcasm\",\n",
      "        \"678\": \"multi-sarcasm\",\n",
      "        \"679\": \"multi-sarcasm\",\n",
      "        \"680\": \"multi-sarcasm\",\n",
      "        \"681\": \"multi-sarcasm\",\n",
      "        \"682\": \"not-sarcasm\",\n",
      "        \"683\": \"multi-sarcasm\",\n",
      "        \"684\": \"multi-sarcasm\",\n",
      "        \"685\": \"multi-sarcasm\",\n",
      "        \"686\": \"multi-sarcasm\",\n",
      "        \"687\": \"multi-sarcasm\",\n",
      "        \"688\": \"multi-sarcasm\",\n",
      "        \"689\": \"multi-sarcasm\",\n",
      "        \"690\": \"not-sarcasm\",\n",
      "        \"691\": \"multi-sarcasm\",\n",
      "        \"692\": \"not-sarcasm\",\n",
      "        \"693\": \"multi-sarcasm\",\n",
      "        \"694\": \"multi-sarcasm\",\n",
      "        \"695\": \"not-sarcasm\",\n",
      "        \"696\": \"multi-sarcasm\",\n",
      "        \"697\": \"not-sarcasm\",\n",
      "        \"698\": \"multi-sarcasm\",\n",
      "        \"699\": \"multi-sarcasm\",\n",
      "        \"700\": \"multi-sarcasm\",\n",
      "        \"701\": \"not-sarcasm\",\n",
      "        \"702\": \"not-sarcasm\",\n",
      "        \"703\": \"not-sarcasm\",\n",
      "        \"704\": \"multi-sarcasm\",\n",
      "        \"705\": \"multi-sarcasm\",\n",
      "        \"706\": \"multi-sarcasm\",\n",
      "        \"707\": \"multi-sarcasm\",\n",
      "        \"708\": \"multi-sarcasm\",\n",
      "        \"709\": \"multi-sarcasm\",\n",
      "        \"710\": \"multi-sarcasm\",\n",
      "        \"711\": \"multi-sarcasm\",\n",
      "        \"712\": \"multi-sarcasm\",\n",
      "        \"713\": \"multi-sarcasm\",\n",
      "        \"714\": \"not-sarcasm\",\n",
      "        \"715\": \"multi-sarcasm\",\n",
      "        \"716\": \"multi-sarcasm\",\n",
      "        \"717\": \"multi-sarcasm\",\n",
      "        \"718\": \"multi-sarcasm\",\n",
      "        \"719\": \"multi-sarcasm\",\n",
      "        \"720\": \"multi-sarcasm\",\n",
      "        \"721\": \"not-sarcasm\",\n",
      "        \"722\": \"not-sarcasm\",\n",
      "        \"723\": \"multi-sarcasm\",\n",
      "        \"724\": \"not-sarcasm\",\n",
      "        \"725\": \"multi-sarcasm\",\n",
      "        \"726\": \"not-sarcasm\",\n",
      "        \"727\": \"multi-sarcasm\",\n",
      "        \"728\": \"multi-sarcasm\",\n",
      "        \"729\": \"not-sarcasm\",\n",
      "        \"730\": \"multi-sarcasm\",\n",
      "        \"731\": \"not-sarcasm\",\n",
      "        \"732\": \"not-sarcasm\",\n",
      "        \"733\": \"multi-sarcasm\",\n",
      "        \"734\": \"multi-sarcasm\",\n",
      "        \"735\": \"multi-sarcasm\",\n",
      "        \"736\": \"not-sarcasm\",\n",
      "        \"737\": \"multi-sarcasm\",\n",
      "        \"738\": \"multi-sarcasm\",\n",
      "        \"739\": \"multi-sarcasm\",\n",
      "        \"740\": \"multi-sarcasm\",\n",
      "        \"741\": \"multi-sarcasm\",\n",
      "        \"742\": \"not-sarcasm\",\n",
      "        \"743\": \"multi-sarcasm\",\n",
      "        \"744\": \"multi-sarcasm\",\n",
      "        \"745\": \"multi-sarcasm\",\n",
      "        \"746\": \"not-sarcasm\",\n",
      "        \"747\": \"multi-sarcasm\",\n",
      "        \"748\": \"multi-sarcasm\",\n",
      "        \"749\": \"not-sarcasm\",\n",
      "        \"750\": \"not-sarcasm\",\n",
      "        \"751\": \"multi-sarcasm\",\n",
      "        \"752\": \"not-sarcasm\",\n",
      "        \"753\": \"multi-sarcasm\",\n",
      "        \"754\": \"multi-sarcasm\",\n",
      "        \"755\": \"not-sarcasm\",\n",
      "        \"756\": \"multi-sarcasm\",\n",
      "        \"757\": \"multi-sarcasm\",\n",
      "        \"758\": \"image-sarcasm\",\n",
      "        \"759\": \"multi-sarcasm\",\n",
      "        \"760\": \"multi-sarcasm\",\n",
      "        \"761\": \"not-sarcasm\",\n",
      "        \"762\": \"not-sarcasm\",\n",
      "        \"763\": \"multi-sarcasm\",\n",
      "        \"764\": \"not-sarcasm\",\n",
      "        \"765\": \"multi-sarcasm\",\n",
      "        \"766\": \"multi-sarcasm\",\n",
      "        \"767\": \"not-sarcasm\",\n",
      "        \"768\": \"multi-sarcasm\",\n",
      "        \"769\": \"multi-sarcasm\",\n",
      "        \"770\": \"multi-sarcasm\",\n",
      "        \"771\": \"multi-sarcasm\",\n",
      "        \"772\": \"not-sarcasm\",\n",
      "        \"773\": \"multi-sarcasm\",\n",
      "        \"774\": \"multi-sarcasm\",\n",
      "        \"775\": \"multi-sarcasm\",\n",
      "        \"776\": \"multi-sarcasm\",\n",
      "        \"777\": \"not-sarcasm\",\n",
      "        \"778\": \"multi-sarcasm\",\n",
      "        \"779\": \"not-sarcasm\",\n",
      "        \"780\": \"multi-sarcasm\",\n",
      "        \"781\": \"multi-sarcasm\",\n",
      "        \"782\": \"multi-sarcasm\",\n",
      "        \"783\": \"not-sarcasm\",\n",
      "        \"784\": \"multi-sarcasm\",\n",
      "        \"785\": \"image-sarcasm\",\n",
      "        \"786\": \"not-sarcasm\",\n",
      "        \"787\": \"multi-sarcasm\",\n",
      "        \"788\": \"not-sarcasm\",\n",
      "        \"789\": \"not-sarcasm\",\n",
      "        \"790\": \"not-sarcasm\",\n",
      "        \"791\": \"multi-sarcasm\",\n",
      "        \"792\": \"multi-sarcasm\",\n",
      "        \"793\": \"multi-sarcasm\",\n",
      "        \"794\": \"multi-sarcasm\",\n",
      "        \"795\": \"not-sarcasm\",\n",
      "        \"796\": \"multi-sarcasm\",\n",
      "        \"797\": \"multi-sarcasm\",\n",
      "        \"798\": \"multi-sarcasm\",\n",
      "        \"799\": \"not-sarcasm\",\n",
      "        \"800\": \"multi-sarcasm\",\n",
      "        \"801\": \"multi-sarcasm\",\n",
      "        \"802\": \"multi-sarcasm\",\n",
      "        \"803\": \"not-sarcasm\",\n",
      "        \"804\": \"not-sarcasm\",\n",
      "        \"805\": \"multi-sarcasm\",\n",
      "        \"806\": \"multi-sarcasm\",\n",
      "        \"807\": \"not-sarcasm\",\n",
      "        \"808\": \"multi-sarcasm\",\n",
      "        \"809\": \"not-sarcasm\",\n",
      "        \"810\": \"multi-sarcasm\",\n",
      "        \"811\": \"multi-sarcasm\",\n",
      "        \"812\": \"multi-sarcasm\",\n",
      "        \"813\": \"multi-sarcasm\",\n",
      "        \"814\": \"not-sarcasm\",\n",
      "        \"815\": \"multi-sarcasm\",\n",
      "        \"816\": \"multi-sarcasm\",\n",
      "        \"817\": \"multi-sarcasm\",\n",
      "        \"818\": \"multi-sarcasm\",\n",
      "        \"819\": \"not-sarcasm\",\n",
      "        \"820\": \"multi-sarcasm\",\n",
      "        \"821\": \"multi-sarcasm\",\n",
      "        \"822\": \"multi-sarcasm\",\n",
      "        \"823\": \"not-sarcasm\",\n",
      "        \"824\": \"not-sarcasm\",\n",
      "        \"825\": \"not-sarcasm\",\n",
      "        \"826\": \"not-sarcasm\",\n",
      "        \"827\": \"not-sarcasm\",\n",
      "        \"828\": \"multi-sarcasm\",\n",
      "        \"829\": \"not-sarcasm\",\n",
      "        \"830\": \"multi-sarcasm\",\n",
      "        \"831\": \"image-sarcasm\",\n",
      "        \"832\": \"multi-sarcasm\",\n",
      "        \"833\": \"not-sarcasm\",\n",
      "        \"834\": \"not-sarcasm\",\n",
      "        \"835\": \"multi-sarcasm\",\n",
      "        \"836\": \"multi-sarcasm\",\n",
      "        \"837\": \"multi-sarcasm\",\n",
      "        \"838\": \"not-sarcasm\",\n",
      "        \"839\": \"multi-sarcasm\",\n",
      "        \"840\": \"multi-sarcasm\",\n",
      "        \"841\": \"not-sarcasm\",\n",
      "        \"842\": \"multi-sarcasm\",\n",
      "        \"843\": \"multi-sarcasm\",\n",
      "        \"844\": \"multi-sarcasm\",\n",
      "        \"845\": \"multi-sarcasm\",\n",
      "        \"846\": \"multi-sarcasm\",\n",
      "        \"847\": \"multi-sarcasm\",\n",
      "        \"848\": \"multi-sarcasm\",\n",
      "        \"849\": \"not-sarcasm\",\n",
      "        \"850\": \"not-sarcasm\",\n",
      "        \"851\": \"not-sarcasm\",\n",
      "        \"852\": \"not-sarcasm\",\n",
      "        \"853\": \"multi-sarcasm\",\n",
      "        \"854\": \"multi-sarcasm\",\n",
      "        \"855\": \"multi-sarcasm\",\n",
      "        \"856\": \"not-sarcasm\",\n",
      "        \"857\": \"multi-sarcasm\",\n",
      "        \"858\": \"multi-sarcasm\",\n",
      "        \"859\": \"multi-sarcasm\",\n",
      "        \"860\": \"multi-sarcasm\",\n",
      "        \"861\": \"not-sarcasm\",\n",
      "        \"862\": \"multi-sarcasm\",\n",
      "        \"863\": \"multi-sarcasm\",\n",
      "        \"864\": \"not-sarcasm\",\n",
      "        \"865\": \"multi-sarcasm\",\n",
      "        \"866\": \"multi-sarcasm\",\n",
      "        \"867\": \"not-sarcasm\",\n",
      "        \"868\": \"not-sarcasm\",\n",
      "        \"869\": \"multi-sarcasm\",\n",
      "        \"870\": \"multi-sarcasm\",\n",
      "        \"871\": \"image-sarcasm\",\n",
      "        \"872\": \"multi-sarcasm\",\n",
      "        \"873\": \"multi-sarcasm\",\n",
      "        \"874\": \"multi-sarcasm\",\n",
      "        \"875\": \"multi-sarcasm\",\n",
      "        \"876\": \"not-sarcasm\",\n",
      "        \"877\": \"multi-sarcasm\",\n",
      "        \"878\": \"multi-sarcasm\",\n",
      "        \"879\": \"not-sarcasm\",\n",
      "        \"880\": \"multi-sarcasm\",\n",
      "        \"881\": \"multi-sarcasm\",\n",
      "        \"882\": \"multi-sarcasm\",\n",
      "        \"883\": \"not-sarcasm\",\n",
      "        \"884\": \"multi-sarcasm\",\n",
      "        \"885\": \"not-sarcasm\",\n",
      "        \"886\": \"multi-sarcasm\",\n",
      "        \"887\": \"not-sarcasm\",\n",
      "        \"888\": \"multi-sarcasm\",\n",
      "        \"889\": \"multi-sarcasm\",\n",
      "        \"890\": \"multi-sarcasm\",\n",
      "        \"891\": \"multi-sarcasm\",\n",
      "        \"892\": \"not-sarcasm\",\n",
      "        \"893\": \"not-sarcasm\",\n",
      "        \"894\": \"multi-sarcasm\",\n",
      "        \"895\": \"multi-sarcasm\",\n",
      "        \"896\": \"multi-sarcasm\",\n",
      "        \"897\": \"multi-sarcasm\",\n",
      "        \"898\": \"multi-sarcasm\",\n",
      "        \"899\": \"multi-sarcasm\",\n",
      "        \"900\": \"multi-sarcasm\",\n",
      "        \"901\": \"not-sarcasm\",\n",
      "        \"902\": \"multi-sarcasm\",\n",
      "        \"903\": \"multi-sarcasm\",\n",
      "        \"904\": \"multi-sarcasm\",\n",
      "        \"905\": \"image-sarcasm\",\n",
      "        \"906\": \"not-sarcasm\",\n",
      "        \"907\": \"multi-sarcasm\",\n",
      "        \"908\": \"multi-sarcasm\",\n",
      "        \"909\": \"multi-sarcasm\",\n",
      "        \"910\": \"not-sarcasm\",\n",
      "        \"911\": \"multi-sarcasm\",\n",
      "        \"912\": \"multi-sarcasm\",\n",
      "        \"913\": \"not-sarcasm\",\n",
      "        \"914\": \"multi-sarcasm\",\n",
      "        \"915\": \"multi-sarcasm\",\n",
      "        \"916\": \"multi-sarcasm\",\n",
      "        \"917\": \"not-sarcasm\",\n",
      "        \"918\": \"not-sarcasm\",\n",
      "        \"919\": \"multi-sarcasm\",\n",
      "        \"920\": \"multi-sarcasm\",\n",
      "        \"921\": \"multi-sarcasm\",\n",
      "        \"922\": \"multi-sarcasm\",\n",
      "        \"923\": \"not-sarcasm\",\n",
      "        \"924\": \"multi-sarcasm\",\n",
      "        \"925\": \"not-sarcasm\",\n",
      "        \"926\": \"not-sarcasm\",\n",
      "        \"927\": \"not-sarcasm\",\n",
      "        \"928\": \"not-sarcasm\",\n",
      "        \"929\": \"not-sarcasm\",\n",
      "        \"930\": \"multi-sarcasm\",\n",
      "        \"931\": \"not-sarcasm\",\n",
      "        \"932\": \"multi-sarcasm\",\n",
      "        \"933\": \"not-sarcasm\",\n",
      "        \"934\": \"multi-sarcasm\",\n",
      "        \"935\": \"multi-sarcasm\",\n",
      "        \"936\": \"multi-sarcasm\",\n",
      "        \"937\": \"multi-sarcasm\",\n",
      "        \"938\": \"multi-sarcasm\",\n",
      "        \"939\": \"multi-sarcasm\",\n",
      "        \"940\": \"multi-sarcasm\",\n",
      "        \"941\": \"multi-sarcasm\",\n",
      "        \"942\": \"not-sarcasm\",\n",
      "        \"943\": \"not-sarcasm\",\n",
      "        \"944\": \"multi-sarcasm\",\n",
      "        \"945\": \"multi-sarcasm\",\n",
      "        \"946\": \"multi-sarcasm\",\n",
      "        \"947\": \"multi-sarcasm\",\n",
      "        \"948\": \"multi-sarcasm\",\n",
      "        \"949\": \"not-sarcasm\",\n",
      "        \"950\": \"multi-sarcasm\",\n",
      "        \"951\": \"multi-sarcasm\",\n",
      "        \"952\": \"not-sarcasm\",\n",
      "        \"953\": \"multi-sarcasm\",\n",
      "        \"954\": \"multi-sarcasm\",\n",
      "        \"955\": \"multi-sarcasm\",\n",
      "        \"956\": \"not-sarcasm\",\n",
      "        \"957\": \"multi-sarcasm\",\n",
      "        \"958\": \"multi-sarcasm\",\n",
      "        \"959\": \"not-sarcasm\",\n",
      "        \"960\": \"not-sarcasm\",\n",
      "        \"961\": \"not-sarcasm\",\n",
      "        \"962\": \"not-sarcasm\",\n",
      "        \"963\": \"not-sarcasm\",\n",
      "        \"964\": \"multi-sarcasm\",\n",
      "        \"965\": \"multi-sarcasm\",\n",
      "        \"966\": \"multi-sarcasm\",\n",
      "        \"967\": \"multi-sarcasm\",\n",
      "        \"968\": \"multi-sarcasm\",\n",
      "        \"969\": \"not-sarcasm\",\n",
      "        \"970\": \"not-sarcasm\",\n",
      "        \"971\": \"multi-sarcasm\",\n",
      "        \"972\": \"multi-sarcasm\",\n",
      "        \"973\": \"image-sarcasm\",\n",
      "        \"974\": \"multi-sarcasm\",\n",
      "        \"975\": \"multi-sarcasm\",\n",
      "        \"976\": \"multi-sarcasm\",\n",
      "        \"977\": \"multi-sarcasm\",\n",
      "        \"978\": \"not-sarcasm\",\n",
      "        \"979\": \"multi-sarcasm\",\n",
      "        \"980\": \"multi-sarcasm\",\n",
      "        \"981\": \"multi-sarcasm\",\n",
      "        \"982\": \"multi-sarcasm\",\n",
      "        \"983\": \"multi-sarcasm\",\n",
      "        \"984\": \"multi-sarcasm\",\n",
      "        \"985\": \"multi-sarcasm\",\n",
      "        \"986\": \"multi-sarcasm\",\n",
      "        \"987\": \"multi-sarcasm\",\n",
      "        \"988\": \"multi-sarcasm\",\n",
      "        \"989\": \"multi-sarcasm\",\n",
      "        \"990\": \"multi-sarcasm\",\n",
      "        \"991\": \"multi-sarcasm\",\n",
      "        \"992\": \"multi-sarcasm\",\n",
      "        \"993\": \"multi-sarcasm\",\n",
      "        \"994\": \"not-sarcasm\",\n",
      "        \"995\": \"multi-sarcasm\",\n",
      "        \"996\": \"multi-sarcasm\",\n",
      "        \"997\": \"multi-sarcasm\",\n",
      "        \"998\": \"multi-sarcasm\",\n",
      "        \"999\": \"not-sarcasm\",\n",
      "        \"1000\": \"multi-sarcasm\",\n",
      "        \"1001\": \"multi-sarcasm\",\n",
      "        \"1002\": \"multi-sarcasm\",\n",
      "        \"1003\": \"multi-sarcasm\",\n",
      "        \"1004\": \"not-sarcasm\",\n",
      "        \"1005\": \"multi-sarcasm\",\n",
      "        \"1006\": \"not-sarcasm\",\n",
      "        \"1007\": \"not-sarcasm\",\n",
      "        \"1008\": \"multi-sarcasm\",\n",
      "        \"1009\": \"multi-sarcasm\",\n",
      "        \"1010\": \"multi-sarcasm\",\n",
      "        \"1011\": \"multi-sarcasm\",\n",
      "        \"1012\": \"not-sarcasm\",\n",
      "        \"1013\": \"multi-sarcasm\",\n",
      "        \"1014\": \"multi-sarcasm\",\n",
      "        \"1015\": \"multi-sarcasm\",\n",
      "        \"1016\": \"multi-sarcasm\",\n",
      "        \"1017\": \"not-sarcasm\",\n",
      "        \"1018\": \"multi-sarcasm\",\n",
      "        \"1019\": \"not-sarcasm\",\n",
      "        \"1020\": \"multi-sarcasm\",\n",
      "        \"1021\": \"multi-sarcasm\",\n",
      "        \"1022\": \"multi-sarcasm\",\n",
      "        \"1023\": \"not-sarcasm\",\n",
      "        \"1024\": \"multi-sarcasm\",\n",
      "        \"1025\": \"multi-sarcasm\",\n",
      "        \"1026\": \"multi-sarcasm\",\n",
      "        \"1027\": \"multi-sarcasm\",\n",
      "        \"1028\": \"multi-sarcasm\",\n",
      "        \"1029\": \"multi-sarcasm\",\n",
      "        \"1030\": \"multi-sarcasm\",\n",
      "        \"1031\": \"multi-sarcasm\",\n",
      "        \"1032\": \"not-sarcasm\",\n",
      "        \"1033\": \"not-sarcasm\",\n",
      "        \"1034\": \"multi-sarcasm\",\n",
      "        \"1035\": \"multi-sarcasm\",\n",
      "        \"1036\": \"not-sarcasm\",\n",
      "        \"1037\": \"multi-sarcasm\",\n",
      "        \"1038\": \"not-sarcasm\",\n",
      "        \"1039\": \"not-sarcasm\",\n",
      "        \"1040\": \"not-sarcasm\",\n",
      "        \"1041\": \"not-sarcasm\",\n",
      "        \"1042\": \"multi-sarcasm\",\n",
      "        \"1043\": \"not-sarcasm\",\n",
      "        \"1044\": \"multi-sarcasm\",\n",
      "        \"1045\": \"not-sarcasm\",\n",
      "        \"1046\": \"multi-sarcasm\",\n",
      "        \"1047\": \"multi-sarcasm\",\n",
      "        \"1048\": \"multi-sarcasm\",\n",
      "        \"1049\": \"not-sarcasm\",\n",
      "        \"1050\": \"not-sarcasm\",\n",
      "        \"1051\": \"multi-sarcasm\",\n",
      "        \"1052\": \"multi-sarcasm\",\n",
      "        \"1053\": \"not-sarcasm\",\n",
      "        \"1054\": \"multi-sarcasm\",\n",
      "        \"1055\": \"multi-sarcasm\",\n",
      "        \"1056\": \"multi-sarcasm\",\n",
      "        \"1057\": \"multi-sarcasm\",\n",
      "        \"1058\": \"not-sarcasm\",\n",
      "        \"1059\": \"multi-sarcasm\",\n",
      "        \"1060\": \"multi-sarcasm\",\n",
      "        \"1061\": \"not-sarcasm\",\n",
      "        \"1062\": \"multi-sarcasm\",\n",
      "        \"1063\": \"multi-sarcasm\",\n",
      "        \"1064\": \"multi-sarcasm\",\n",
      "        \"1065\": \"multi-sarcasm\",\n",
      "        \"1066\": \"multi-sarcasm\",\n",
      "        \"1067\": \"not-sarcasm\",\n",
      "        \"1068\": \"multi-sarcasm\",\n",
      "        \"1069\": \"not-sarcasm\",\n",
      "        \"1070\": \"multi-sarcasm\",\n",
      "        \"1071\": \"multi-sarcasm\",\n",
      "        \"1072\": \"not-sarcasm\",\n",
      "        \"1073\": \"multi-sarcasm\",\n",
      "        \"1074\": \"multi-sarcasm\",\n",
      "        \"1075\": \"multi-sarcasm\",\n",
      "        \"1076\": \"not-sarcasm\",\n",
      "        \"1077\": \"multi-sarcasm\",\n",
      "        \"1078\": \"multi-sarcasm\",\n",
      "        \"1079\": \"not-sarcasm\",\n",
      "        \"1080\": \"not-sarcasm\",\n",
      "        \"1081\": \"multi-sarcasm\",\n",
      "        \"1082\": \"multi-sarcasm\",\n",
      "        \"1083\": \"multi-sarcasm\",\n",
      "        \"1084\": \"multi-sarcasm\",\n",
      "        \"1085\": \"multi-sarcasm\",\n",
      "        \"1086\": \"multi-sarcasm\",\n",
      "        \"1087\": \"multi-sarcasm\",\n",
      "        \"1088\": \"multi-sarcasm\",\n",
      "        \"1089\": \"multi-sarcasm\",\n",
      "        \"1090\": \"not-sarcasm\",\n",
      "        \"1091\": \"multi-sarcasm\",\n",
      "        \"1092\": \"multi-sarcasm\",\n",
      "        \"1093\": \"not-sarcasm\",\n",
      "        \"1094\": \"multi-sarcasm\",\n",
      "        \"1095\": \"multi-sarcasm\",\n",
      "        \"1096\": \"multi-sarcasm\",\n",
      "        \"1097\": \"multi-sarcasm\",\n",
      "        \"1098\": \"multi-sarcasm\",\n",
      "        \"1099\": \"multi-sarcasm\",\n",
      "        \"1100\": \"multi-sarcasm\",\n",
      "        \"1101\": \"multi-sarcasm\",\n",
      "        \"1102\": \"not-sarcasm\",\n",
      "        \"1103\": \"multi-sarcasm\",\n",
      "        \"1104\": \"multi-sarcasm\",\n",
      "        \"1105\": \"not-sarcasm\",\n",
      "        \"1106\": \"not-sarcasm\",\n",
      "        \"1107\": \"multi-sarcasm\",\n",
      "        \"1108\": \"multi-sarcasm\",\n",
      "        \"1109\": \"multi-sarcasm\",\n",
      "        \"1110\": \"not-sarcasm\",\n",
      "        \"1111\": \"multi-sarcasm\",\n",
      "        \"1112\": \"multi-sarcasm\",\n",
      "        \"1113\": \"multi-sarcasm\",\n",
      "        \"1114\": \"multi-sarcasm\",\n",
      "        \"1115\": \"multi-sarcasm\",\n",
      "        \"1116\": \"multi-sarcasm\",\n",
      "        \"1117\": \"multi-sarcasm\",\n",
      "        \"1118\": \"multi-sarcasm\",\n",
      "        \"1119\": \"multi-sarcasm\",\n",
      "        \"1120\": \"not-sarcasm\",\n",
      "        \"1121\": \"multi-sarcasm\",\n",
      "        \"1122\": \"multi-sarcasm\",\n",
      "        \"1123\": \"multi-sarcasm\",\n",
      "        \"1124\": \"multi-sarcasm\",\n",
      "        \"1125\": \"multi-sarcasm\",\n",
      "        \"1126\": \"not-sarcasm\",\n",
      "        \"1127\": \"multi-sarcasm\",\n",
      "        \"1128\": \"not-sarcasm\",\n",
      "        \"1129\": \"multi-sarcasm\",\n",
      "        \"1130\": \"multi-sarcasm\",\n",
      "        \"1131\": \"multi-sarcasm\",\n",
      "        \"1132\": \"multi-sarcasm\",\n",
      "        \"1133\": \"not-sarcasm\",\n",
      "        \"1134\": \"multi-sarcasm\",\n",
      "        \"1135\": \"not-sarcasm\",\n",
      "        \"1136\": \"multi-sarcasm\",\n",
      "        \"1137\": \"multi-sarcasm\",\n",
      "        \"1138\": \"multi-sarcasm\",\n",
      "        \"1139\": \"multi-sarcasm\",\n",
      "        \"1140\": \"multi-sarcasm\",\n",
      "        \"1141\": \"not-sarcasm\",\n",
      "        \"1142\": \"multi-sarcasm\",\n",
      "        \"1143\": \"multi-sarcasm\",\n",
      "        \"1144\": \"multi-sarcasm\",\n",
      "        \"1145\": \"not-sarcasm\",\n",
      "        \"1146\": \"not-sarcasm\",\n",
      "        \"1147\": \"not-sarcasm\",\n",
      "        \"1148\": \"not-sarcasm\",\n",
      "        \"1149\": \"not-sarcasm\",\n",
      "        \"1150\": \"not-sarcasm\",\n",
      "        \"1151\": \"multi-sarcasm\",\n",
      "        \"1152\": \"multi-sarcasm\",\n",
      "        \"1153\": \"not-sarcasm\",\n",
      "        \"1154\": \"multi-sarcasm\",\n",
      "        \"1155\": \"multi-sarcasm\",\n",
      "        \"1156\": \"multi-sarcasm\",\n",
      "        \"1157\": \"multi-sarcasm\",\n",
      "        \"1158\": \"multi-sarcasm\",\n",
      "        \"1159\": \"multi-sarcasm\",\n",
      "        \"1160\": \"not-sarcasm\",\n",
      "        \"1161\": \"multi-sarcasm\",\n",
      "        \"1162\": \"not-sarcasm\",\n",
      "        \"1163\": \"multi-sarcasm\",\n",
      "        \"1164\": \"not-sarcasm\",\n",
      "        \"1165\": \"multi-sarcasm\",\n",
      "        \"1166\": \"multi-sarcasm\",\n",
      "        \"1167\": \"multi-sarcasm\",\n",
      "        \"1168\": \"not-sarcasm\",\n",
      "        \"1169\": \"multi-sarcasm\",\n",
      "        \"1170\": \"multi-sarcasm\",\n",
      "        \"1171\": \"multi-sarcasm\",\n",
      "        \"1172\": \"multi-sarcasm\",\n",
      "        \"1173\": \"not-sarcasm\",\n",
      "        \"1174\": \"not-sarcasm\",\n",
      "        \"1175\": \"multi-sarcasm\",\n",
      "        \"1176\": \"multi-sarcasm\",\n",
      "        \"1177\": \"multi-sarcasm\",\n",
      "        \"1178\": \"multi-sarcasm\",\n",
      "        \"1179\": \"not-sarcasm\",\n",
      "        \"1180\": \"not-sarcasm\",\n",
      "        \"1181\": \"multi-sarcasm\",\n",
      "        \"1182\": \"multi-sarcasm\",\n",
      "        \"1183\": \"multi-sarcasm\",\n",
      "        \"1184\": \"multi-sarcasm\",\n",
      "        \"1185\": \"multi-sarcasm\",\n",
      "        \"1186\": \"not-sarcasm\",\n",
      "        \"1187\": \"multi-sarcasm\",\n",
      "        \"1188\": \"multi-sarcasm\",\n",
      "        \"1189\": \"multi-sarcasm\",\n",
      "        \"1190\": \"multi-sarcasm\",\n",
      "        \"1191\": \"multi-sarcasm\",\n",
      "        \"1192\": \"not-sarcasm\",\n",
      "        \"1193\": \"multi-sarcasm\",\n",
      "        \"1194\": \"multi-sarcasm\",\n",
      "        \"1195\": \"multi-sarcasm\",\n",
      "        \"1196\": \"multi-sarcasm\",\n",
      "        \"1197\": \"multi-sarcasm\",\n",
      "        \"1198\": \"not-sarcasm\",\n",
      "        \"1199\": \"not-sarcasm\",\n",
      "        \"1200\": \"multi-sarcasm\",\n",
      "        \"1201\": \"not-sarcasm\",\n",
      "        \"1202\": \"not-sarcasm\",\n",
      "        \"1203\": \"multi-sarcasm\",\n",
      "        \"1204\": \"multi-sarcasm\",\n",
      "        \"1205\": \"multi-sarcasm\",\n",
      "        \"1206\": \"multi-sarcasm\",\n",
      "        \"1207\": \"multi-sarcasm\",\n",
      "        \"1208\": \"multi-sarcasm\",\n",
      "        \"1209\": \"multi-sarcasm\",\n",
      "        \"1210\": \"multi-sarcasm\",\n",
      "        \"1211\": \"multi-sarcasm\",\n",
      "        \"1212\": \"multi-sarcasm\",\n",
      "        \"1213\": \"multi-sarcasm\",\n",
      "        \"1214\": \"multi-sarcasm\",\n",
      "        \"1215\": \"multi-sarcasm\",\n",
      "        \"1216\": \"multi-sarcasm\",\n",
      "        \"1217\": \"multi-sarcasm\",\n",
      "        \"1218\": \"multi-sarcasm\",\n",
      "        \"1219\": \"not-sarcasm\",\n",
      "        \"1220\": \"multi-sarcasm\",\n",
      "        \"1221\": \"multi-sarcasm\",\n",
      "        \"1222\": \"multi-sarcasm\",\n",
      "        \"1223\": \"multi-sarcasm\",\n",
      "        \"1224\": \"multi-sarcasm\",\n",
      "        \"1225\": \"multi-sarcasm\",\n",
      "        \"1226\": \"multi-sarcasm\",\n",
      "        \"1227\": \"multi-sarcasm\",\n",
      "        \"1228\": \"multi-sarcasm\",\n",
      "        \"1229\": \"multi-sarcasm\",\n",
      "        \"1230\": \"multi-sarcasm\",\n",
      "        \"1231\": \"multi-sarcasm\",\n",
      "        \"1232\": \"multi-sarcasm\",\n",
      "        \"1233\": \"multi-sarcasm\",\n",
      "        \"1234\": \"not-sarcasm\",\n",
      "        \"1235\": \"multi-sarcasm\",\n",
      "        \"1236\": \"multi-sarcasm\",\n",
      "        \"1237\": \"not-sarcasm\",\n",
      "        \"1238\": \"multi-sarcasm\",\n",
      "        \"1239\": \"multi-sarcasm\",\n",
      "        \"1240\": \"multi-sarcasm\",\n",
      "        \"1241\": \"multi-sarcasm\",\n",
      "        \"1242\": \"multi-sarcasm\",\n",
      "        \"1243\": \"multi-sarcasm\",\n",
      "        \"1244\": \"multi-sarcasm\",\n",
      "        \"1245\": \"multi-sarcasm\",\n",
      "        \"1246\": \"not-sarcasm\",\n",
      "        \"1247\": \"multi-sarcasm\",\n",
      "        \"1248\": \"multi-sarcasm\",\n",
      "        \"1249\": \"multi-sarcasm\",\n",
      "        \"1250\": \"multi-sarcasm\",\n",
      "        \"1251\": \"multi-sarcasm\",\n",
      "        \"1252\": \"multi-sarcasm\",\n",
      "        \"1253\": \"multi-sarcasm\",\n",
      "        \"1254\": \"multi-sarcasm\",\n",
      "        \"1255\": \"not-sarcasm\",\n",
      "        \"1256\": \"multi-sarcasm\",\n",
      "        \"1257\": \"multi-sarcasm\",\n",
      "        \"1258\": \"image-sarcasm\",\n",
      "        \"1259\": \"multi-sarcasm\",\n",
      "        \"1260\": \"multi-sarcasm\",\n",
      "        \"1261\": \"not-sarcasm\",\n",
      "        \"1262\": \"not-sarcasm\",\n",
      "        \"1263\": \"multi-sarcasm\",\n",
      "        \"1264\": \"multi-sarcasm\",\n",
      "        \"1265\": \"multi-sarcasm\",\n",
      "        \"1266\": \"multi-sarcasm\",\n",
      "        \"1267\": \"not-sarcasm\",\n",
      "        \"1268\": \"not-sarcasm\",\n",
      "        \"1269\": \"image-sarcasm\",\n",
      "        \"1270\": \"multi-sarcasm\",\n",
      "        \"1271\": \"multi-sarcasm\",\n",
      "        \"1272\": \"multi-sarcasm\",\n",
      "        \"1273\": \"multi-sarcasm\",\n",
      "        \"1274\": \"multi-sarcasm\",\n",
      "        \"1275\": \"not-sarcasm\",\n",
      "        \"1276\": \"multi-sarcasm\",\n",
      "        \"1277\": \"multi-sarcasm\",\n",
      "        \"1278\": \"multi-sarcasm\",\n",
      "        \"1279\": \"multi-sarcasm\",\n",
      "        \"1280\": \"multi-sarcasm\",\n",
      "        \"1281\": \"multi-sarcasm\",\n",
      "        \"1282\": \"multi-sarcasm\",\n",
      "        \"1283\": \"multi-sarcasm\",\n",
      "        \"1284\": \"multi-sarcasm\",\n",
      "        \"1285\": \"not-sarcasm\",\n",
      "        \"1286\": \"multi-sarcasm\",\n",
      "        \"1287\": \"multi-sarcasm\",\n",
      "        \"1288\": \"multi-sarcasm\",\n",
      "        \"1289\": \"multi-sarcasm\",\n",
      "        \"1290\": \"multi-sarcasm\",\n",
      "        \"1291\": \"multi-sarcasm\",\n",
      "        \"1292\": \"multi-sarcasm\",\n",
      "        \"1293\": \"multi-sarcasm\",\n",
      "        \"1294\": \"not-sarcasm\",\n",
      "        \"1295\": \"multi-sarcasm\",\n",
      "        \"1296\": \"multi-sarcasm\",\n",
      "        \"1297\": \"multi-sarcasm\",\n",
      "        \"1298\": \"not-sarcasm\",\n",
      "        \"1299\": \"not-sarcasm\",\n",
      "        \"1300\": \"multi-sarcasm\",\n",
      "        \"1301\": \"multi-sarcasm\",\n",
      "        \"1302\": \"not-sarcasm\",\n",
      "        \"1303\": \"multi-sarcasm\",\n",
      "        \"1304\": \"multi-sarcasm\",\n",
      "        \"1305\": \"multi-sarcasm\",\n",
      "        \"1306\": \"multi-sarcasm\",\n",
      "        \"1307\": \"multi-sarcasm\",\n",
      "        \"1308\": \"multi-sarcasm\",\n",
      "        \"1309\": \"multi-sarcasm\",\n",
      "        \"1310\": \"multi-sarcasm\",\n",
      "        \"1311\": \"multi-sarcasm\",\n",
      "        \"1312\": \"multi-sarcasm\",\n",
      "        \"1313\": \"multi-sarcasm\",\n",
      "        \"1314\": \"multi-sarcasm\",\n",
      "        \"1315\": \"multi-sarcasm\",\n",
      "        \"1316\": \"multi-sarcasm\",\n",
      "        \"1317\": \"multi-sarcasm\",\n",
      "        \"1318\": \"multi-sarcasm\",\n",
      "        \"1319\": \"image-sarcasm\",\n",
      "        \"1320\": \"multi-sarcasm\",\n",
      "        \"1321\": \"multi-sarcasm\",\n",
      "        \"1322\": \"multi-sarcasm\",\n",
      "        \"1323\": \"not-sarcasm\",\n",
      "        \"1324\": \"multi-sarcasm\",\n",
      "        \"1325\": \"multi-sarcasm\",\n",
      "        \"1326\": \"multi-sarcasm\",\n",
      "        \"1327\": \"multi-sarcasm\",\n",
      "        \"1328\": \"multi-sarcasm\",\n",
      "        \"1329\": \"multi-sarcasm\",\n",
      "        \"1330\": \"multi-sarcasm\",\n",
      "        \"1331\": \"multi-sarcasm\",\n",
      "        \"1332\": \"multi-sarcasm\",\n",
      "        \"1333\": \"multi-sarcasm\",\n",
      "        \"1334\": \"multi-sarcasm\",\n",
      "        \"1335\": \"multi-sarcasm\",\n",
      "        \"1336\": \"multi-sarcasm\",\n",
      "        \"1337\": \"multi-sarcasm\",\n",
      "        \"1338\": \"multi-sarcasm\",\n",
      "        \"1339\": \"not-sarcasm\",\n",
      "        \"1340\": \"multi-sarcasm\",\n",
      "        \"1341\": \"multi-sarcasm\",\n",
      "        \"1342\": \"multi-sarcasm\",\n",
      "        \"1343\": \"multi-sarcasm\",\n",
      "        \"1344\": \"multi-sarcasm\",\n",
      "        \"1345\": \"multi-sarcasm\",\n",
      "        \"1346\": \"multi-sarcasm\",\n",
      "        \"1347\": \"multi-sarcasm\",\n",
      "        \"1348\": \"multi-sarcasm\",\n",
      "        \"1349\": \"multi-sarcasm\",\n",
      "        \"1350\": \"multi-sarcasm\",\n",
      "        \"1351\": \"multi-sarcasm\",\n",
      "        \"1352\": \"multi-sarcasm\",\n",
      "        \"1353\": \"multi-sarcasm\",\n",
      "        \"1354\": \"not-sarcasm\",\n",
      "        \"1355\": \"not-sarcasm\",\n",
      "        \"1356\": \"multi-sarcasm\",\n",
      "        \"1357\": \"multi-sarcasm\",\n",
      "        \"1358\": \"multi-sarcasm\",\n",
      "        \"1359\": \"multi-sarcasm\",\n",
      "        \"1360\": \"not-sarcasm\",\n",
      "        \"1361\": \"multi-sarcasm\",\n",
      "        \"1362\": \"multi-sarcasm\",\n",
      "        \"1363\": \"multi-sarcasm\",\n",
      "        \"1364\": \"multi-sarcasm\",\n",
      "        \"1365\": \"multi-sarcasm\",\n",
      "        \"1366\": \"multi-sarcasm\",\n",
      "        \"1367\": \"multi-sarcasm\",\n",
      "        \"1368\": \"multi-sarcasm\",\n",
      "        \"1369\": \"multi-sarcasm\",\n",
      "        \"1370\": \"multi-sarcasm\",\n",
      "        \"1371\": \"not-sarcasm\",\n",
      "        \"1372\": \"multi-sarcasm\",\n",
      "        \"1373\": \"multi-sarcasm\",\n",
      "        \"1374\": \"not-sarcasm\",\n",
      "        \"1375\": \"multi-sarcasm\",\n",
      "        \"1376\": \"multi-sarcasm\",\n",
      "        \"1377\": \"multi-sarcasm\",\n",
      "        \"1378\": \"multi-sarcasm\",\n",
      "        \"1379\": \"multi-sarcasm\",\n",
      "        \"1380\": \"multi-sarcasm\",\n",
      "        \"1381\": \"multi-sarcasm\",\n",
      "        \"1382\": \"not-sarcasm\",\n",
      "        \"1383\": \"multi-sarcasm\",\n",
      "        \"1384\": \"not-sarcasm\",\n",
      "        \"1385\": \"multi-sarcasm\",\n",
      "        \"1386\": \"multi-sarcasm\",\n",
      "        \"1387\": \"multi-sarcasm\",\n",
      "        \"1388\": \"multi-sarcasm\",\n",
      "        \"1389\": \"multi-sarcasm\",\n",
      "        \"1390\": \"multi-sarcasm\",\n",
      "        \"1391\": \"multi-sarcasm\",\n",
      "        \"1392\": \"multi-sarcasm\",\n",
      "        \"1393\": \"multi-sarcasm\",\n",
      "        \"1394\": \"not-sarcasm\",\n",
      "        \"1395\": \"multi-sarcasm\",\n",
      "        \"1396\": \"multi-sarcasm\",\n",
      "        \"1397\": \"multi-sarcasm\",\n",
      "        \"1398\": \"multi-sarcasm\",\n",
      "        \"1399\": \"multi-sarcasm\",\n",
      "        \"1400\": \"multi-sarcasm\",\n",
      "        \"1401\": \"multi-sarcasm\",\n",
      "        \"1402\": \"multi-sarcasm\",\n",
      "        \"1403\": \"image-sarcasm\",\n",
      "        \"1404\": \"multi-sarcasm\",\n",
      "        \"1405\": \"multi-sarcasm\",\n",
      "        \"1406\": \"multi-sarcasm\",\n",
      "        \"1407\": \"multi-sarcasm\",\n",
      "        \"1408\": \"multi-sarcasm\",\n",
      "        \"1409\": \"multi-sarcasm\",\n",
      "        \"1410\": \"multi-sarcasm\",\n",
      "        \"1411\": \"multi-sarcasm\",\n",
      "        \"1412\": \"multi-sarcasm\"\n",
      "    },\n",
      "    \"phase\": \"dev\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results = {df_public.index[i]: predicted_label_names[i] for i in range(len(predicted_label_names))}\n",
    "\n",
    "# Tạo cấu trúc JSON\n",
    "output = {\n",
    "    \"results\": results,\n",
    "    \"phase\": \"dev\"\n",
    "}\n",
    "\n",
    "# Chuyển đổi sang chuỗi JSON\n",
    "json_output = json.dumps(output, indent=4)\n",
    "\n",
    "# In ra kết quả\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.json', 'w') as json_file:\n",
    "    json.dump(output, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
